{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhIEdwVON7CRrXUWKtL0HA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lenishu/IPA_using_Densenet/blob/main/SLP_weight_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTEBIJOZSgRI",
        "outputId": "df8d7a9b-ae43-4c92-e406-ad8db629cfe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Running MLP matrix analysis with 50.0% pruning on ALL layers, batch size 60000\n",
            "Analyzing initial state (batch 0)...\n",
            "Analyzing after first batch...\n",
            "Completed batch 1/100\n",
            "Completed batch 2/100\n",
            "Completed batch 3/100\n",
            "Completed batch 4/100\n",
            "Completed batch 5/100\n",
            "Completed batch 6/100\n",
            "Completed batch 7/100\n",
            "Completed batch 8/100\n",
            "Completed batch 9/100\n",
            "Completed batch 10/100\n",
            "Completed batch 11/100\n",
            "Completed batch 12/100\n",
            "Completed batch 13/100\n",
            "Completed batch 14/100\n",
            "Completed batch 15/100\n",
            "Completed batch 16/100\n",
            "Completed batch 17/100\n",
            "Completed batch 18/100\n",
            "Completed batch 19/100\n",
            "Completed batch 20/100\n",
            "Completed batch 21/100\n",
            "Completed batch 22/100\n",
            "Completed batch 23/100\n",
            "Completed batch 24/100\n",
            "Completed batch 25/100\n",
            "Completed batch 26/100\n",
            "Completed batch 27/100\n",
            "Completed batch 28/100\n",
            "Completed batch 29/100\n",
            "Completed batch 30/100\n",
            "Completed batch 31/100\n",
            "Completed batch 32/100\n",
            "Completed batch 33/100\n",
            "Completed batch 34/100\n",
            "Completed batch 35/100\n",
            "Completed batch 36/100\n",
            "Completed batch 37/100\n",
            "Completed batch 38/100\n",
            "Completed batch 39/100\n",
            "Completed batch 40/100\n",
            "Completed batch 41/100\n",
            "Completed batch 42/100\n",
            "Completed batch 43/100\n",
            "Completed batch 44/100\n",
            "Completed batch 45/100\n",
            "Completed batch 46/100\n",
            "Completed batch 47/100\n",
            "Completed batch 48/100\n",
            "Completed batch 49/100\n",
            "Completed batch 50/100\n",
            "Completed batch 51/100\n",
            "Completed batch 52/100\n",
            "Completed batch 53/100\n",
            "Completed batch 54/100\n",
            "Completed batch 55/100\n",
            "Completed batch 56/100\n",
            "Completed batch 57/100\n",
            "Completed batch 58/100\n",
            "Completed batch 59/100\n",
            "Completed batch 60/100\n",
            "Completed batch 61/100\n",
            "Completed batch 62/100\n",
            "Completed batch 63/100\n",
            "Completed batch 64/100\n",
            "Completed batch 65/100\n",
            "Completed batch 66/100\n",
            "Completed batch 67/100\n",
            "Completed batch 68/100\n",
            "Completed batch 69/100\n",
            "Completed batch 70/100\n",
            "Completed batch 71/100\n",
            "Completed batch 72/100\n",
            "Completed batch 73/100\n",
            "Completed batch 74/100\n",
            "Completed batch 75/100\n",
            "Completed batch 76/100\n",
            "Completed batch 77/100\n",
            "Completed batch 78/100\n",
            "Completed batch 79/100\n",
            "Completed batch 80/100\n",
            "Completed batch 81/100\n",
            "Completed batch 82/100\n",
            "Completed batch 83/100\n",
            "Completed batch 84/100\n",
            "Completed batch 85/100\n",
            "Completed batch 86/100\n",
            "Completed batch 87/100\n",
            "Completed batch 88/100\n",
            "Completed batch 89/100\n",
            "Completed batch 90/100\n",
            "Completed batch 91/100\n",
            "Completed batch 92/100\n",
            "Completed batch 93/100\n",
            "Completed batch 94/100\n",
            "Completed batch 95/100\n",
            "Completed batch 96/100\n",
            "Completed batch 97/100\n",
            "Completed batch 98/100\n",
            "Completed batch 99/100\n",
            "Analyzing after 100 batches...\n",
            "Completed batch 100/100\n",
            "\n",
            "MLP Matrix analysis complete! Check the 'mlp_matrix_analysis_prune_0.5_batch_60000' directory for results.\n",
            "Files created:\n",
            "- matrix_analysis_batch_0.txt (initial state)\n",
            "- matrix_analysis_batch_1.txt (after first batch)\n",
            "- matrix_analysis_batch_100.txt (after 100 batches)\n",
            "- matrices/ folder with files:\n",
            "  * .npy files (NumPy format for loading back)\n",
            "  * _flattened.txt (all weights in single column)\n",
            "  * _structured.txt (weights organized by output class)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.utils.prune as prune\n",
        "import numpy as np\n",
        "\n",
        "# Define a simple fully connected model (SLP) with pruning\n",
        "class PrunedMLP(nn.Module):\n",
        "    def __init__(self, prune_percentage=50.0, Prune_Layers='ALL'):\n",
        "        super(PrunedMLP, self).__init__()\n",
        "        # Flatten the 28x28 input images (MNIST)\n",
        "        self.flatten = nn.Flatten()\n",
        "        # Define a single fully connected layer (input to output)\n",
        "        self.fc1 = nn.Linear(28 * 28, 10, bias=False)  # Input to output (10 classes for MNIST)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Prune the model based on the specified layers\n",
        "        self.prune_model(prune_percentage, Prune_Layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "    def prune_model(self, prune_percentage, Prune_Layers):\n",
        "        for name, module in self.named_modules():\n",
        "            if Prune_Layers == 'ALL' and isinstance(module, nn.Linear):\n",
        "                prune.random_unstructured(module, name='weight', amount=prune_percentage)\n",
        "\n",
        "def analyze_layer_matrices(model, batch_num, analysis_dir):\n",
        "    \"\"\"\n",
        "    Analyze and save matrix information for each layer\n",
        "    \"\"\"\n",
        "    os.makedirs(analysis_dir, exist_ok=True)\n",
        "\n",
        "    analysis_file = os.path.join(analysis_dir, f\"matrix_analysis_batch_{batch_num}.txt\")\n",
        "\n",
        "    with open(analysis_file, 'w') as f:\n",
        "        f.write(f\"=== MLP MATRIX ANALYSIS - BATCH {batch_num} ===\\n\\n\")\n",
        "\n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                # Get the weight matrix (handling pruned weights)\n",
        "                if hasattr(module, 'weight_mask'):\n",
        "                    # If pruned, get the actual weights (masked)\n",
        "                    # Ensure mask is on the same device as weights\n",
        "                    mask = module.weight_mask.to(module.weight.device)\n",
        "                    weights = module.weight * mask\n",
        "                    f.write(f\"Layer: {name} (PRUNED)\\n\")\n",
        "                else:\n",
        "                    weights = module.weight\n",
        "                    f.write(f\"Layer: {name} (NOT PRUNED)\\n\")\n",
        "\n",
        "                # Convert to numpy for analysis\n",
        "                weight_np = weights.detach().cpu().numpy()\n",
        "\n",
        "                # Matrix shape information\n",
        "                f.write(f\"  Matrix Shape: {weight_np.shape}\\n\")\n",
        "                f.write(f\"  Total Parameters: {weight_np.size}\\n\")\n",
        "\n",
        "                # Value range analysis\n",
        "                min_val = np.min(weight_np)\n",
        "                max_val = np.max(weight_np)\n",
        "                mean_val = np.mean(weight_np)\n",
        "                std_val = np.std(weight_np)\n",
        "\n",
        "                f.write(f\"  Value Range: [{min_val:.6f}, {max_val:.6f}]\\n\")\n",
        "                f.write(f\"  Mean: {mean_val:.6f}\\n\")\n",
        "                f.write(f\"  Std Dev: {std_val:.6f}\\n\")\n",
        "\n",
        "                # Count zero values (important for pruned networks)\n",
        "                zero_count = np.sum(weight_np == 0)\n",
        "                zero_percentage = (zero_count / weight_np.size) * 100\n",
        "                f.write(f\"  Zero Values: {zero_count}/{weight_np.size} ({zero_percentage:.2f}%)\\n\")\n",
        "\n",
        "                # Additional statistics for MLP\n",
        "                non_zero_weights = weight_np[weight_np != 0]\n",
        "                if len(non_zero_weights) > 0:\n",
        "                    f.write(f\"  Non-zero Mean: {np.mean(non_zero_weights):.6f}\\n\")\n",
        "                    f.write(f\"  Non-zero Std: {np.std(non_zero_weights):.6f}\\n\")\n",
        "                    f.write(f\"  Non-zero Range: [{np.min(non_zero_weights):.6f}, {np.max(non_zero_weights):.6f}]\\n\")\n",
        "\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"=== MLP LAYER SHAPE SUMMARY ===\\n\")\n",
        "        f.write(\"fc1.weight: [10, 784] = 10 output classes, 784 input pixels (28*28)\\n\")\n",
        "        f.write(\"This is a Single Layer Perceptron (SLP) - direct pixel to class mapping\\n\\n\")\n",
        "\n",
        "def save_layer_matrices(model, batch_num, matrices_dir):\n",
        "    \"\"\"\n",
        "    Save the actual matrix values to separate files\n",
        "    \"\"\"\n",
        "    os.makedirs(matrices_dir, exist_ok=True)\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Get the weight matrix\n",
        "            if hasattr(module, 'weight_mask'):\n",
        "                # Ensure mask is on the same device as weights\n",
        "                mask = module.weight_mask.to(module.weight.device)\n",
        "                weights = module.weight * mask\n",
        "            else:\n",
        "                weights = module.weight\n",
        "\n",
        "            weight_np = weights.detach().cpu().numpy()\n",
        "\n",
        "            # Save matrix to file\n",
        "            filename = os.path.join(matrices_dir, f\"{name}_weights_batch_{batch_num}.npy\")\n",
        "            np.save(filename, weight_np)\n",
        "\n",
        "            # Also save as text file for readability (flattened)\n",
        "            txt_filename = os.path.join(matrices_dir, f\"{name}_weights_batch_{batch_num}_flattened.txt\")\n",
        "            np.savetxt(txt_filename, weight_np.reshape(-1), fmt='%.8f')\n",
        "\n",
        "            # Save as structured text file preserving matrix shape\n",
        "            structured_txt_filename = os.path.join(matrices_dir, f\"{name}_weights_batch_{batch_num}_structured.txt\")\n",
        "            with open(structured_txt_filename, 'w') as txt_file:\n",
        "                txt_file.write(f\"# Layer: {name}\\n\")\n",
        "                txt_file.write(f\"# Shape: {weight_np.shape}\\n\")\n",
        "                txt_file.write(f\"# Batch: {batch_num}\\n\")\n",
        "                txt_file.write(f\"# MLP Weight Matrix: 10 output classes × 784 input pixels\\n\")\n",
        "                txt_file.write(\"#\" + \"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "                # For MLP, show weights organized by output class\n",
        "                txt_file.write(\"Weight Matrix (Output Classes × Input Pixels):\\n\\n\")\n",
        "\n",
        "                for class_idx in range(weight_np.shape[0]):  # 10 classes\n",
        "                    txt_file.write(f\"Class {class_idx} weights (784 pixel connections):\\n\")\n",
        "\n",
        "                    # Show first 20×20 pixels of the 28×28 image for readability\n",
        "                    pixel_weights = weight_np[class_idx].reshape(28, 28)  # Reshape to image format\n",
        "\n",
        "                    txt_file.write(\"  First 20×20 pixels:\\n\")\n",
        "                    for row in range(min(20, 28)):\n",
        "                        txt_file.write(\"  \")\n",
        "                        for col in range(min(20, 28)):\n",
        "                            txt_file.write(f\"{pixel_weights[row, col]:8.4f} \")\n",
        "                        if 28 > 20:\n",
        "                            txt_file.write(\"...\")\n",
        "                        txt_file.write(\"\\n\")\n",
        "\n",
        "                    if 28 > 20:\n",
        "                        txt_file.write(\"  ...\\n\")\n",
        "\n",
        "                    # Statistics for this class\n",
        "                    class_weights = weight_np[class_idx]\n",
        "                    zero_count = np.sum(class_weights == 0)\n",
        "                    txt_file.write(f\"  Class {class_idx} stats: min={np.min(class_weights):.4f}, \")\n",
        "                    txt_file.write(f\"max={np.max(class_weights):.4f}, \")\n",
        "                    txt_file.write(f\"zeros={zero_count}/784 ({zero_count/784*100:.1f}%)\\n\\n\")\n",
        "\n",
        "def train_with_analysis(model, train_loader, criterion, optimizer, fraction_of_epoch, device, analysis_dir):\n",
        "    \"\"\"\n",
        "    Training function focused only on matrix analysis\n",
        "    \"\"\"\n",
        "    # Create directories for analysis\n",
        "    matrices_dir = os.path.join(analysis_dir, \"matrices\")\n",
        "\n",
        "    # ANALYZE INITIAL STATE (batch 0)\n",
        "    print(\"Analyzing initial state (batch 0)...\")\n",
        "    analyze_layer_matrices(model, 0, analysis_dir)\n",
        "    save_layer_matrices(model, 0, matrices_dir)\n",
        "\n",
        "    model.train()\n",
        "    batch_number = 1\n",
        "    total_batches = len(train_loader)\n",
        "    target_batches = int(fraction_of_epoch * total_batches)\n",
        "\n",
        "    for epoch in range(100):\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            if batch_number > target_batches:\n",
        "                print(f\"Stopping after processing {batch_number} batches\")\n",
        "                return\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # ANALYZE AFTER FIRST BATCH\n",
        "            if batch_number == 1:\n",
        "                print(\"Analyzing after first batch...\")\n",
        "                analyze_layer_matrices(model, 1, analysis_dir)\n",
        "                save_layer_matrices(model, 1, matrices_dir)\n",
        "\n",
        "            # ANALYZE AFTER 100 BATCHES\n",
        "            if batch_number == 100:\n",
        "                print(\"Analyzing after 100 batches...\")\n",
        "                analyze_layer_matrices(model, 100, analysis_dir)\n",
        "                save_layer_matrices(model, 100, matrices_dir)\n",
        "\n",
        "            print(f\"Completed batch {batch_number}/{target_batches}\")\n",
        "            batch_number += 1\n",
        "\n",
        "def calculate_epochs_for_batch_size(batch_size):\n",
        "    \"\"\"\n",
        "    Calculate number of epochs based on the batch size.\n",
        "    \"\"\"\n",
        "    if batch_size == 64:\n",
        "        return 1\n",
        "    elif batch_size == 256:\n",
        "        return 1\n",
        "    elif batch_size == 4000:\n",
        "        return 10\n",
        "    elif batch_size == 60000:\n",
        "        return 100\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported batch size\")\n",
        "\n",
        "def main():\n",
        "    CUDA = True\n",
        "    device = torch.device(\"cuda\" if CUDA and torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load MNIST dataset\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "\n",
        "    # For demonstration, let's run one configuration\n",
        "    prune_percentage = 0.5  # 50% pruning\n",
        "    Prune_Layers = 'ALL'\n",
        "    batch_size = 60000\n",
        "\n",
        "    print(f\"Running MLP matrix analysis with {prune_percentage*100}% pruning on {Prune_Layers} layers, batch size {batch_size}\")\n",
        "\n",
        "    # Create model\n",
        "    model = PrunedMLP(prune_percentage, Prune_Layers).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adadelta(model.parameters())\n",
        "\n",
        "    # Create data loader (only need training data)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Create output directories\n",
        "    analysis_dir = f\"mlp_matrix_analysis_prune_{prune_percentage}_batch_{batch_size}\"\n",
        "    os.makedirs(analysis_dir, exist_ok=True)\n",
        "\n",
        "    # Calculate epochs\n",
        "    epochs = calculate_epochs_for_batch_size(batch_size)\n",
        "\n",
        "    # Train with matrix analysis only\n",
        "    train_with_analysis(model, train_loader, criterion, optimizer,\n",
        "                       fraction_of_epoch=epochs, device=device, analysis_dir=analysis_dir)\n",
        "\n",
        "    print(f\"\\nMLP Matrix analysis complete! Check the '{analysis_dir}' directory for results.\")\n",
        "    print(\"Files created:\")\n",
        "    print(\"- matrix_analysis_batch_0.txt (initial state)\")\n",
        "    print(\"- matrix_analysis_batch_1.txt (after first batch)\")\n",
        "    print(\"- matrix_analysis_batch_100.txt (after 100 batches)\")\n",
        "    print(\"- matrices/ folder with files:\")\n",
        "    print(\"  * .npy files (NumPy format for loading back)\")\n",
        "    print(\"  * _flattened.txt (all weights in single column)\")\n",
        "    print(\"  * _structured.txt (weights organized by output class)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "95f1p4zdTj88"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}