{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "testing_v_7pruning_0_gradient_updated_weights_densenet.ipynb",
      "authorship_tag": "ABX9TyPg17eOOjhyUIytZoxFw+Us",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lenishu/IPA_using_Densenet/blob/main/testing_v_7pruning_0_gradient_updated_weights_densenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZUumOk654-l",
        "outputId": "83f22621-cfb4-4373-a8c0-cbdb28faa59e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted. Files will be saved to /content/drive/My Drive/DenseNet_Pruning_Verification\n",
            "Created directory: /content/drive/My Drive/DenseNet_Pruning_Verification/pruning_verification\n",
            "Test mode: Pruning exactly 10 parameters (approx. 0.0001351485% of 7,399,616 parameters)\n",
            "Total trainable parameters: 7,399,616\n",
            "Parameters to prune: 10 (0.00013514851%)\n",
            "Saved initial weights data to /content/drive/My Drive/DenseNet_Pruning_Verification/pruning_verification/weights_tracking/initial_weights_data.txt\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 2s/step - accuracy: 0.1045 - loss: 6.3766 - val_accuracy: 0.1100 - val_loss: 10571005437149184.0000\n",
            "Saved all pruned weights tracking data to /content/drive/My Drive/DenseNet_Pruning_Verification/pruning_verification/weights_tracking/pruned_weights_tracking.csv\n",
            "Saved all pruned flat weight tracking to /content/drive/My Drive/DenseNet_Pruning_Verification/pruning_verification/weights_tracking/all_flat_pruned_weights.txt\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.layers import Input, BatchNormalization, ReLU, Conv2D, Dense, MaxPool2D, AvgPool2D, GlobalAvgPool2D, Concatenate\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10, cifar100\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Model\n",
        "import math\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Mount Google Drive (this will work in Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    GOOGLE_DRIVE_MOUNTED = True\n",
        "    GOOGLE_DRIVE_BASE_PATH = \"/content/drive/My Drive/DenseNet_Pruning_Verification\"\n",
        "    # Create base directory in Google Drive if it doesn't exist\n",
        "    os.makedirs(GOOGLE_DRIVE_BASE_PATH, exist_ok=True)\n",
        "    print(f\"Google Drive mounted. Files will be saved to {GOOGLE_DRIVE_BASE_PATH}\")\n",
        "except ImportError:\n",
        "    GOOGLE_DRIVE_MOUNTED = False\n",
        "    GOOGLE_DRIVE_BASE_PATH = None\n",
        "    print(\"Not running in Colab or Google Drive not available. Files will be saved locally.\")\n",
        "\n",
        "# Configuration\n",
        "BATCH_SIZE_TRAIN = 64\n",
        "BATCH_SIZE_TEST = 256\n",
        "LEARNING_RATE = 0.1\n",
        "EPOCHS_PER_RUN = 1\n",
        "prune_percentage= 0.00013514851\n",
        "\n",
        "# Function to get output directory path (either on Google Drive or locally)\n",
        "def get_output_dir(local_dir, create=True):\n",
        "    \"\"\"\n",
        "    Get the output directory path (either on Google Drive or locally)\n",
        "\n",
        "    Parameters:\n",
        "    local_dir -- Local directory path\n",
        "    create -- Whether to create the directory if it doesn't exist\n",
        "\n",
        "    Returns:\n",
        "    Full path to the directory\n",
        "    \"\"\"\n",
        "    if GOOGLE_DRIVE_MOUNTED:\n",
        "        # Extract the base name from the local path if it's a full path\n",
        "        if os.path.isabs(local_dir):\n",
        "            base_name = os.path.basename(local_dir)\n",
        "        else:\n",
        "            base_name = local_dir\n",
        "\n",
        "        # Construct the full path on Google Drive\n",
        "        full_path = os.path.join(GOOGLE_DRIVE_BASE_PATH, base_name)\n",
        "    else:\n",
        "        full_path = local_dir\n",
        "\n",
        "    # Create the directory if requested\n",
        "    if create:\n",
        "        os.makedirs(full_path, exist_ok=True)\n",
        "        print(f\"Created directory: {full_path}\")\n",
        "\n",
        "    return full_path\n",
        "\n",
        "# Function to load and preprocess datasets\n",
        "def load_dataset(dataset_name):\n",
        "    if dataset_name == 'mnist':\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "        # Reshape to add channel dimension and resize to 32x32\n",
        "        x_train = np.pad(x_train, ((0, 0), (2, 2), (2, 2)), 'constant')\n",
        "        x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2)), 'constant')\n",
        "        x_train = np.expand_dims(x_train, axis=-1)\n",
        "        x_test = np.expand_dims(x_test, axis=-1)\n",
        "        # Repeat the channel to match the 3-channel input expected by DenseNet\n",
        "        x_train = np.repeat(x_train, 3, axis=-1)\n",
        "        x_test = np.repeat(x_test, 3, axis=-1)\n",
        "        num_classes = 10\n",
        "    elif dataset_name == 'fashion_mnist':\n",
        "        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "        x_train = np.pad(x_train, ((0, 0), (2, 2), (2, 2)), 'constant')\n",
        "        x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2)), 'constant')\n",
        "        x_train = np.expand_dims(x_train, axis=-1)\n",
        "        x_test = np.expand_dims(x_test, axis=-1)\n",
        "        x_train = np.repeat(x_train, 3, axis=-1)\n",
        "        x_test = np.repeat(x_test, 3, axis=-1)\n",
        "        num_classes = 10\n",
        "    elif dataset_name == 'cifar10':\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "        num_classes = 10\n",
        "    elif dataset_name == 'cifar100':\n",
        "        (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "        num_classes = 100\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "\n",
        "    # Normalize data\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "    # Convert class vectors to binary class matrices\n",
        "    y_train = to_categorical(y_train, num_classes)\n",
        "    y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test), num_classes\n",
        "\n",
        "# Define DenseNet-121 architecture\n",
        "def bn_rl_conv(x, filters, kernel_size):\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv2D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
        "    return x\n",
        "\n",
        "def dense_block(tensor, k, reps):\n",
        "    for _ in range(reps):\n",
        "        x = bn_rl_conv(tensor, filters=4 * k, kernel_size=1)\n",
        "        x = bn_rl_conv(x, filters=k, kernel_size=3)\n",
        "        tensor = Concatenate()([tensor, x])\n",
        "    return tensor\n",
        "\n",
        "def transition_layer(x, theta):\n",
        "    f = int(tf.keras.backend.int_shape(x)[-1] * theta)\n",
        "    x = bn_rl_conv(x, filters=f, kernel_size=1)\n",
        "    x = AvgPool2D(pool_size=2, strides=2, padding='same')(x)\n",
        "    return x\n",
        "\n",
        "def create_densenet121(input_shape, num_classes, k=32, theta=0.5):\n",
        "    # DenseNet-121 has repetitions [6, 12, 24, 16]\n",
        "    repetitions = [6, 12, 24, 16]\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(2 * k, 7, strides=2, padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "    for reps in repetitions:\n",
        "        x = dense_block(x, k, reps)\n",
        "        x = transition_layer(x, theta)\n",
        "\n",
        "    x = GlobalAvgPool2D()(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "\n",
        "# Enhanced MaskWeightsCallback with tracking of weight values, gradients, and updates\n",
        "# Here's an updated version of the EnhancedMaskWeightsCallback class that captures\n",
        "# the initial random weights before any training happens\n",
        "\n",
        "# Here's the complete EnhancedMaskWeightsCallback class with all required methods\n",
        "\n",
        "\n",
        "# Here's the updated EnhancedMaskWeightsCallback class with renumbered batches\n",
        "# The main changes are in the save_all_flat_pruned_data_txt method\n",
        "# Here's the enhanced EnhancedMaskWeightsCallback class with complete weight tracking\n",
        "# We'll capture the weights before pruning, after pruning, gradients, and after updates\n",
        "\n",
        "class CompleteWeightTrackingCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, masks, layer_indices, pruned_indices_per_layer, all_pruned_flat_indices, output_dir):\n",
        "        super().__init__()\n",
        "        self.masks = masks\n",
        "        self.layer_indices = layer_indices\n",
        "        self.pruned_indices_per_layer = pruned_indices_per_layer\n",
        "        self.all_pruned_flat_indices = all_pruned_flat_indices\n",
        "        self.output_dir = output_dir\n",
        "        self.batch_count = 0\n",
        "        self.max_batches_to_verify = 4  # Track batches 0, 1, 2, 3 (will be displayed as 1, 2, 3, 4)\n",
        "\n",
        "        # Store values for each batch\n",
        "        self.pre_pruning_weights = None  # Will store weights before any pruning\n",
        "        self.initial_weights = None      # Will store weights after pruning but before any training\n",
        "        self.weight_values = {}          # Will store weights at the beginning of each batch\n",
        "        self.gradient_values = {}        # Will store gradients for each batch\n",
        "        self.updated_weight_values = {}  # Will store weights after update but before re-pruning\n",
        "\n",
        "        # Create the weights directory\n",
        "        weights_dir = os.path.join(output_dir, \"weights_tracking\")\n",
        "        os.makedirs(weights_dir, exist_ok=True)\n",
        "        self.weights_dir = weights_dir\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        # Capture the weights after pruning but before any training\n",
        "        self.initial_weights = []\n",
        "        for layer_idx in self.layer_indices:\n",
        "            layer = self.model.layers[layer_idx]\n",
        "            weights = layer.get_weights()[0]\n",
        "            self.initial_weights.append(weights.copy())\n",
        "\n",
        "        # Save the initial weights data\n",
        "        self.save_initial_weights_data()\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        if self.batch_count < self.max_batches_to_verify:\n",
        "            # Get all the weight tensors before updates\n",
        "            weights_before = []\n",
        "            for layer_idx in self.layer_indices:\n",
        "                layer = self.model.layers[layer_idx]\n",
        "                weights_before.append(layer.get_weights()[0])\n",
        "\n",
        "            # Store them for this batch\n",
        "            self.weight_values[self.batch_count] = weights_before\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        if self.batch_count < self.max_batches_to_verify:\n",
        "            # Get updated weights after the batch\n",
        "            updated_weights = []\n",
        "            for layer_idx in self.layer_indices:\n",
        "                layer = self.model.layers[layer_idx]\n",
        "                weights = layer.get_weights()[0]\n",
        "                updated_weights.append(weights.copy())  # Copy before masking\n",
        "\n",
        "            # Store the updated weights\n",
        "            self.updated_weight_values[self.batch_count] = updated_weights\n",
        "\n",
        "            # Try to extract gradients - depends on model setup\n",
        "            try:\n",
        "                gradients = []\n",
        "                # Calculate approximate gradients based on weight changes\n",
        "                # (updated_weights - original_weights) / learning_rate\n",
        "                for i, layer_idx in enumerate(self.layer_indices):\n",
        "                    grad = (updated_weights[i] - self.weight_values[self.batch_count][i]) / LEARNING_RATE\n",
        "                    gradients.append(grad)\n",
        "\n",
        "                self.gradient_values[self.batch_count] = gradients\n",
        "            except:\n",
        "                print(f\"Couldn't extract gradients for batch {self.batch_count}\")\n",
        "                self.gradient_values[self.batch_count] = [None] * len(self.layer_indices)\n",
        "\n",
        "            # Apply masks after data collection\n",
        "            for i, layer_idx in enumerate(self.layer_indices):\n",
        "                layer = self.model.layers[layer_idx]\n",
        "                weights = layer.get_weights()\n",
        "                weights[0] = weights[0] * self.masks[i]  # Apply mask\n",
        "                layer.set_weights(weights)\n",
        "\n",
        "            # Increment batch counter\n",
        "            self.batch_count += 1\n",
        "\n",
        "    def set_pre_pruning_weights(self, weights):\n",
        "        \"\"\"\n",
        "        Set the pre-pruning weights (called from the pruning function).\n",
        "        \"\"\"\n",
        "        self.pre_pruning_weights = weights\n",
        "\n",
        "    def save_initial_weights_data(self):\n",
        "        \"\"\"\n",
        "        Save the initial weights data before and after pruning.\n",
        "        \"\"\"\n",
        "        filename = \"initial_weights_data.txt\"\n",
        "        filepath = os.path.join(self.weights_dir, filename)\n",
        "\n",
        "        with open(filepath, 'w') as f:\n",
        "            f.write(\"Initial Weights Data Before and After Pruning\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "            if self.pre_pruning_weights is None:\n",
        "                f.write(\"Pre-pruning weights not captured. Please make sure to set them using set_pre_pruning_weights().\\n\\n\")\n",
        "            else:\n",
        "                f.write(\"Flat Index | Pre-Pruning Value | After Pruning Value\\n\")\n",
        "                f.write(\"-\" * 70 + \"\\n\")\n",
        "\n",
        "                # Process each pruned index\n",
        "                params_so_far = 0\n",
        "                for i, layer_idx in enumerate(self.layer_indices):\n",
        "                    layer = self.model.layers[layer_idx]\n",
        "                    layer_name = layer.name\n",
        "\n",
        "                    # Get the weights for this layer\n",
        "                    pre_weights = self.pre_pruning_weights[i]\n",
        "                    post_weights = self.initial_weights[i]\n",
        "\n",
        "                    # Flatten the weights for this layer\n",
        "                    flat_pre_weights = pre_weights.flatten()\n",
        "                    flat_post_weights = post_weights.flatten()\n",
        "\n",
        "                    # Write pruned indices for this layer\n",
        "                    f.write(f\"\\nLayer {i} ({layer_name}):\\n\")\n",
        "\n",
        "                    # Get pruned indices for this layer\n",
        "                    layer_pruned_indices = self.pruned_indices_per_layer.get(i, [])\n",
        "\n",
        "                    for idx in layer_pruned_indices:\n",
        "                        flat_idx = params_so_far + idx\n",
        "                        pre_val = flat_pre_weights[idx]\n",
        "                        post_val = flat_post_weights[idx]\n",
        "                        f.write(f\"{flat_idx:10d} | {pre_val:15.10f} | {post_val:15.10f}\\n\")\n",
        "\n",
        "                    # Update params counter\n",
        "                    params_so_far += pre_weights.size\n",
        "\n",
        "        print(f\"Saved initial weights data to {filepath}\")\n",
        "\n",
        "    def save_all_data_to_csv(self):\n",
        "        \"\"\"\n",
        "        Save all tracked data to a comprehensive CSV file for easier analysis.\n",
        "        \"\"\"\n",
        "        csv_path = os.path.join(self.weights_dir, \"pruned_weights_tracking.csv\")\n",
        "\n",
        "        with open(csv_path, 'w', newline='') as csv_file:\n",
        "            writer = csv.writer(csv_file)\n",
        "\n",
        "            # Header with all states we're tracking\n",
        "            writer.writerow([\n",
        "                'batch', 'flat_index', 'layer_index',\n",
        "                'pre_pruning_value', 'initial_value',\n",
        "                'weight_before_batch', 'gradient', 'weight_after_update'\n",
        "            ])\n",
        "\n",
        "            # Pre-pruning and initial weights (batch 0)\n",
        "            if self.initial_weights is not None:\n",
        "                params_so_far = 0\n",
        "                for i, layer_idx in enumerate(self.layer_indices):\n",
        "                    post_weights = self.initial_weights[i]\n",
        "                    flat_post_weights = post_weights.flatten()\n",
        "\n",
        "                    # Get pruned indices for this layer\n",
        "                    layer_pruned_indices = self.pruned_indices_per_layer.get(i, [])\n",
        "\n",
        "                    for idx in layer_pruned_indices:\n",
        "                        flat_idx = params_so_far + idx\n",
        "                        init_weight = flat_post_weights[idx]\n",
        "\n",
        "                        # Get pre-pruning weight if available\n",
        "                        pre_weight = 'N/A'\n",
        "                        if self.pre_pruning_weights is not None and i < len(self.pre_pruning_weights):\n",
        "                            pre_weight = self.pre_pruning_weights[i].flatten()[idx]\n",
        "\n",
        "                        # Use 0 as batch number to indicate initial state\n",
        "                        writer.writerow([0, flat_idx, i, pre_weight, init_weight, init_weight, 'N/A', 'N/A'])\n",
        "\n",
        "                    params_so_far += post_weights.size\n",
        "\n",
        "            # Process each batch (using batch+1 for the renumbering)\n",
        "            for batch in range(min(self.batch_count, self.max_batches_to_verify)):\n",
        "                # Skip if we don't have data for this batch\n",
        "                if batch not in self.weight_values:\n",
        "                    continue\n",
        "\n",
        "                params_so_far = 0\n",
        "\n",
        "                # Process each layer\n",
        "                for i, layer_idx in enumerate(self.layer_indices):\n",
        "                    # Get all the data\n",
        "                    original_weights = self.weight_values.get(batch, [])[i] if batch in self.weight_values and i < len(self.weight_values[batch]) else None\n",
        "                    gradients = self.gradient_values.get(batch, [])[i] if batch in self.gradient_values and i < len(self.gradient_values[batch]) else None\n",
        "                    updated_weights = self.updated_weight_values.get(batch, [])[i] if batch in self.updated_weight_values and i < len(self.updated_weight_values[batch]) else None\n",
        "\n",
        "                    if original_weights is None or updated_weights is None:\n",
        "                        continue\n",
        "\n",
        "                    # Flatten all arrays\n",
        "                    flat_orig_weights = original_weights.flatten()\n",
        "                    flat_updated_weights = updated_weights.flatten()\n",
        "                    flat_gradients = gradients.flatten() if gradients is not None else None\n",
        "\n",
        "                    # Get pruned indices for this layer\n",
        "                    layer_pruned_indices = self.pruned_indices_per_layer.get(i, [])\n",
        "\n",
        "                    for idx in layer_pruned_indices:\n",
        "                        flat_idx = params_so_far + idx\n",
        "                        orig_weight = flat_orig_weights[idx]\n",
        "                        updated_weight = flat_updated_weights[idx]\n",
        "\n",
        "                        # Get pre-pruning and initial weights\n",
        "                        pre_weight = 'N/A'\n",
        "                        if self.pre_pruning_weights is not None and i < len(self.pre_pruning_weights):\n",
        "                            pre_weight = self.pre_pruning_weights[i].flatten()[idx]\n",
        "\n",
        "                        init_weight = 'N/A'\n",
        "                        if self.initial_weights is not None and i < len(self.initial_weights):\n",
        "                            init_weight = self.initial_weights[i].flatten()[idx]\n",
        "\n",
        "                        gradient = 'N/A'\n",
        "                        if flat_gradients is not None:\n",
        "                            gradient = flat_gradients[idx]\n",
        "\n",
        "                        # Use batch+1 for display (batch 0 becomes batch 1, etc.)\n",
        "                        writer.writerow([\n",
        "                            batch+1, flat_idx, i,\n",
        "                            pre_weight, init_weight,\n",
        "                            orig_weight, gradient, updated_weight\n",
        "                        ])\n",
        "\n",
        "                    # Update params counter\n",
        "                    params_so_far += original_weights.size\n",
        "\n",
        "        print(f\"Saved all pruned weights tracking data to {csv_path}\")\n",
        "\n",
        "    def save_all_flat_pruned_data_txt(self):\n",
        "        \"\"\"\n",
        "        Save all batches' pruned data using only `all_pruned_flat_indices` to a single .txt file.\n",
        "        Includes pre-pruning weights, initial weights, batch weights, gradients, and updated weights.\n",
        "        \"\"\"\n",
        "        txt_path = os.path.join(self.weights_dir, \"all_flat_pruned_weights.txt\")\n",
        "\n",
        "        with open(txt_path, 'w') as f:\n",
        "            f.write(\"Complete Flat Pruned Indices Tracking\\n\")\n",
        "            f.write(\"=\" * 120 + \"\\n\\n\")\n",
        "\n",
        "            # First, add a section for pre-pruning and initial weights\n",
        "            f.write(\"SECTION 1: INITIAL WEIGHTS (BEFORE AND AFTER PRUNING)\\n\")\n",
        "            f.write(\"-\" * 120 + \"\\n\")\n",
        "\n",
        "            if self.pre_pruning_weights is not None:\n",
        "                f.write(\"Flat Index | Pre-Pruning Value (Original Random) | After Pruning Value\\n\")\n",
        "                f.write(\"-\" * 80 + \"\\n\")\n",
        "            else:\n",
        "                f.write(\"Flat Index | After Pruning Value\\n\")\n",
        "                f.write(\"-\" * 50 + \"\\n\")\n",
        "\n",
        "            # Check if we have any pruned indices\n",
        "            if not self.all_pruned_flat_indices:\n",
        "                f.write(\"No pruned indices found. Please check if prune_percentage > 0.\\n\")\n",
        "                print(f\"Warning: No pruned indices found. Please check if prune_percentage > 0.\")\n",
        "                return\n",
        "\n",
        "            # Print initial weights for each pruned index\n",
        "            params_so_far = 0\n",
        "            for i, layer_idx in enumerate(self.layer_indices):\n",
        "                if self.initial_weights is None or i >= len(self.initial_weights):\n",
        "                    continue\n",
        "\n",
        "                layer = self.model.layers[layer_idx]\n",
        "                layer_name = layer.name\n",
        "                init_weights = self.initial_weights[i]\n",
        "                flat_init_weights = init_weights.flatten()\n",
        "                size = init_weights.size\n",
        "\n",
        "                # Get indices in this layer\n",
        "                layer_indices = []\n",
        "                for flat_idx in self.all_pruned_flat_indices:\n",
        "                    if params_so_far <= flat_idx < params_so_far + size:\n",
        "                        local_idx = flat_idx - params_so_far\n",
        "                        layer_indices.append((flat_idx, local_idx))\n",
        "\n",
        "                if layer_indices:\n",
        "                    f.write(f\"\\nLayer {i} ({layer_name}):\\n\")\n",
        "                    for flat_idx, local_idx in layer_indices:\n",
        "                        init_val = flat_init_weights[local_idx]\n",
        "\n",
        "                        if self.pre_pruning_weights is not None and i < len(self.pre_pruning_weights):\n",
        "                            pre_val = self.pre_pruning_weights[i].flatten()[local_idx]\n",
        "                            f.write(f\"{flat_idx:10d} | {pre_val:30.10f} | {init_val:15.10f}\\n\")\n",
        "                        else:\n",
        "                            f.write(f\"{flat_idx:10d} | {init_val:15.10f}\\n\")\n",
        "\n",
        "                params_so_far += size\n",
        "\n",
        "            # Now add a section for batch training tracking\n",
        "            f.write(\"\\n\\nSECTION 2: TRAINING BATCH TRACKING\\n\")\n",
        "            f.write(\"-\" * 120 + \"\\n\")\n",
        "            f.write(\"Flat Index | Batch | Weight Before | Gradient | Weight After Update\\n\")\n",
        "            f.write(\"-\" * 100 + \"\\n\")\n",
        "\n",
        "            for batch in range(min(self.batch_count, self.max_batches_to_verify)):\n",
        "                # Skip if we don't have data for this batch\n",
        "                if batch not in self.weight_values:\n",
        "                    continue\n",
        "\n",
        "                # Use batch+1 for display (batch 0 becomes batch 1, etc.)\n",
        "                f.write(f\"\\nBatch {batch+1}:\\n\")\n",
        "                f.write(\"-\" * 50 + \"\\n\")\n",
        "\n",
        "                for flat_idx in self.all_pruned_flat_indices:\n",
        "                    params_so_far = 0\n",
        "                    for i, layer_idx in enumerate(self.layer_indices):\n",
        "                        # Skip if we don't have data for this layer\n",
        "                        if i >= len(self.weight_values.get(batch, [])):\n",
        "                            continue\n",
        "\n",
        "                        orig_weights = self.weight_values.get(batch, [])[i]\n",
        "                        grads = self.gradient_values.get(batch, [])[i]\n",
        "                        updated_weights = self.updated_weight_values.get(batch, [])[i]\n",
        "\n",
        "                        if orig_weights is None or updated_weights is None:\n",
        "                            continue\n",
        "\n",
        "                        size = orig_weights.size\n",
        "                        if params_so_far <= flat_idx < params_so_far + size:\n",
        "                            local_idx = flat_idx - params_so_far\n",
        "                            orig_val = orig_weights.flatten()[local_idx]\n",
        "\n",
        "                            # Handle gradient values properly\n",
        "                            if grads is not None:\n",
        "                                try:\n",
        "                                    grad_val = grads.flatten()[local_idx]\n",
        "                                    grad_str = f\"{grad_val:15.10f}\"\n",
        "                                except:\n",
        "                                    grad_str = \"N/A\"\n",
        "                            else:\n",
        "                                grad_str = \"N/A\"\n",
        "\n",
        "                            updated_val = updated_weights.flatten()[local_idx]\n",
        "\n",
        "                            # Use batch+1 for display (batch 0 becomes batch 1, etc.)\n",
        "                            f.write(f\"{flat_idx:10d} | {batch+1:5d} | {orig_val:15.10f} | {grad_str:15} | {updated_val:15.10f}\\n\")\n",
        "                            break  # Move to next flat index\n",
        "                        params_so_far += size\n",
        "\n",
        "        print(f\"Saved all pruned flat weight tracking to {txt_path}\")\n",
        "\n",
        "# Update the parameter_mask_pruning_with_tracking function to capture pre-pruning weights\n",
        "\n",
        "def parameter_mask_pruning_with_tracking(model, prune_percentage, seed=None, output_dir=\"pruning_verification\"):\n",
        "    \"\"\"\n",
        "    Randomly select a percentage of weights across the entire network, set them to zero,\n",
        "    and track their values during training.\n",
        "\n",
        "    Parameters:\n",
        "    model -- The Keras model to prune\n",
        "    prune_percentage -- Percentage of weights to prune (0-100)\n",
        "    seed -- Random seed for reproducibility\n",
        "    output_dir -- Directory to save verification files\n",
        "\n",
        "    Returns:\n",
        "    The pruned model with a custom weight mask and tracking callback\n",
        "    \"\"\"\n",
        "    # Get the output directory (either on Google Drive or locally)\n",
        "    output_dir = get_output_dir(output_dir)\n",
        "\n",
        "    # Set random seed if provided\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    # Get all trainable weights in the model\n",
        "    all_weights = []\n",
        "    all_shapes = []\n",
        "    all_layer_indices = []\n",
        "    layer_info = []  # Store layer name and shape for reference\n",
        "\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if isinstance(layer, (Conv2D, Dense)) and len(layer.weights) > 0:\n",
        "            # Only consider weight matrices, not biases\n",
        "            weight = layer.get_weights()[0]\n",
        "            all_weights.append(weight)\n",
        "            all_shapes.append(weight.shape)\n",
        "            all_layer_indices.append(i)\n",
        "            layer_info.append(f\"Layer {i} ({layer.name}): {weight.shape}\")\n",
        "\n",
        "    # Save a copy of the pre-pruning weights\n",
        "    pre_pruning_weights = [w.copy() for w in all_weights]\n",
        "\n",
        "    # Count total parameters\n",
        "    total_params = sum(w.size for w in all_weights)\n",
        "\n",
        "    # Test mode for tiny pruning percentage\n",
        "    if prune_percentage < 0.0002:  # Using a threshold slightly higher than the exact percentage\n",
        "        num_to_prune = 10\n",
        "        print(f\"Test mode: Pruning exactly 10 parameters (approx. {prune_percentage:.10f}% of {total_params:,} parameters)\")\n",
        "    else:\n",
        "        num_to_prune = int(total_params * prune_percentage / 100)\n",
        "        print(f\"Pruning {num_to_prune:,} parameters ({prune_percentage:.2f}% of {total_params:,} parameters)\")\n",
        "\n",
        "    print(f\"Total trainable parameters: {total_params:,}\")\n",
        "    print(f\"Parameters to prune: {num_to_prune:,} ({prune_percentage}%)\")\n",
        "\n",
        "    # Create masks for each layer (initially all ones)\n",
        "    masks = [np.ones_like(w) for w in all_weights]\n",
        "\n",
        "    # Dictionary to track which indices were set to zero in each layer\n",
        "    pruned_indices_per_layer = {}\n",
        "\n",
        "    # Store flat indices for verification\n",
        "    all_pruned_flat_indices = []\n",
        "\n",
        "    # If pruning percentage is not 0, create masks and apply them\n",
        "    if prune_percentage > 0:\n",
        "        # Randomly select indices to prune across all parameters\n",
        "        flat_indices = np.random.choice(total_params, size=num_to_prune, replace=False)\n",
        "        all_pruned_flat_indices = sorted(flat_indices.tolist())\n",
        "\n",
        "        # Map flat indices back to layer, row, col indices\n",
        "        params_so_far = 0\n",
        "        for i, weight in enumerate(all_weights):\n",
        "            size = weight.size\n",
        "            # Get indices that fall within this layer\n",
        "            indices_in_layer = flat_indices[(flat_indices >= params_so_far) &\n",
        "                                          (flat_indices < params_so_far + size)] - params_so_far\n",
        "\n",
        "            # Store these indices for verification\n",
        "            pruned_indices_per_layer[i] = sorted(indices_in_layer.tolist())\n",
        "\n",
        "            # Flatten the mask, set the selected indices to zero, then reshape back\n",
        "            flat_mask = masks[i].flatten()\n",
        "            flat_mask[indices_in_layer] = 0\n",
        "            masks[i] = flat_mask.reshape(all_shapes[i])\n",
        "\n",
        "            params_so_far += size\n",
        "\n",
        "        # Apply masks to each layer's weights\n",
        "        for i, layer_idx in enumerate(all_layer_indices):\n",
        "            layer = model.layers[layer_idx]\n",
        "            weights = layer.get_weights()\n",
        "            weights[0] = weights[0] * masks[i]  # Apply mask to weights\n",
        "            layer.set_weights(weights)\n",
        "\n",
        "    # Write pruned indices to file\n",
        "    with open(os.path.join(output_dir, \"prune-indices_batch-0.txt\"), 'w') as f:\n",
        "        f.write(f\"Total parameters: {total_params}\\n\")\n",
        "        f.write(f\"Total pruned: {num_to_prune}\\n\\n\")\n",
        "        f.write(\"Layer information:\\n\")\n",
        "        for info in layer_info:\n",
        "            f.write(f\"{info}\\n\")\n",
        "        f.write(\"\\nPruned indices per layer:\\n\")\n",
        "        for layer_idx, indices in pruned_indices_per_layer.items():\n",
        "            f.write(f\"Layer index {layer_idx}: {len(indices)} pruned indices\\n\")\n",
        "            if len(indices) <= 20:  # Only print all indices if there are few\n",
        "                f.write(f\"  {indices}\\n\")\n",
        "            else:\n",
        "                f.write(f\"  First 10: {indices[:10]}\\n\")\n",
        "                f.write(f\"  Last 10: {indices[-10:]}\\n\")\n",
        "\n",
        "        f.write(\"\\nAll pruned flat indices:\\n\")\n",
        "        # Print all pruned indices\n",
        "        f.write(f\"{all_pruned_flat_indices}\\n\")\n",
        "\n",
        "    # Also save the full list to a separate CSV file for easier processing\n",
        "    csv_path = os.path.join(output_dir, \"pruned_indices.csv\")\n",
        "    with open(csv_path, 'w', newline='') as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow(['flat_index'])  # Header\n",
        "        for idx in all_pruned_flat_indices:\n",
        "            writer.writerow([idx])\n",
        "\n",
        "    # Attach the enhanced mask callback to the model for weight tracking\n",
        "    mask_callback = CompleteWeightTrackingCallback(\n",
        "        masks,\n",
        "        all_layer_indices,\n",
        "        pruned_indices_per_layer,\n",
        "        all_pruned_flat_indices,\n",
        "        output_dir\n",
        "    )\n",
        "\n",
        "    # Set the pre-pruning weights in the callback\n",
        "    mask_callback.set_pre_pruning_weights(pre_pruning_weights)\n",
        "\n",
        "    model.mask_callback = mask_callback\n",
        "\n",
        "    # Reset random seed\n",
        "    if seed is not None:\n",
        "        np.random.seed(None)\n",
        "\n",
        "    return model\n",
        "\n",
        "#=======================================================================================================================\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "(x_train, y_train), (x_test, y_test), num_classes = load_dataset('cifar10')\n",
        "\n",
        "# Create model\n",
        "model = create_densenet121(input_shape=(32, 32, 3), num_classes=num_classes)\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE, momentum=0.9),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Apply pruning with enhanced tracking\n",
        "model = parameter_mask_pruning_with_tracking(model, prune_percentage=prune_percentage, seed=42)\n",
        "\n",
        "# Custom callback to save all tracking data at the end of training\n",
        "class SaveTrackingDataCallback(tf.keras.callbacks.Callback):\n",
        "    def on_train_end(self, logs=None):\n",
        "        if hasattr(model, 'mask_callback'):\n",
        "            model.mask_callback.save_all_data_to_csv()\n",
        "            model.mask_callback.save_all_flat_pruned_data_txt()\n",
        "\n",
        "\n",
        "x_train = x_train[:1000]\n",
        "y_train = y_train[:1000]\n",
        "x_test = x_test[:100]\n",
        "y_test = y_test[:100]\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=BATCH_SIZE_TRAIN,\n",
        "    epochs=EPOCHS_PER_RUN,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[model.mask_callback, SaveTrackingDataCallback()]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zbH96HHP6AT-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}