{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lenishu/IPA_using_Densenet/blob/main/Version_2_Pruning_Densenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRHbmKz5wNkV",
        "outputId": "7848ff42-22ba-4c3a-a262-3f431aea8408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available datasets:\n",
            "1: mnist\n",
            "2: fashion_mnist\n",
            "3: cifar10\n",
            "4: cifar100\n",
            "Select dataset (1-4) or press Enter for MNIST: 1\n",
            "Running DenseNet-121 pruning experiments on mnist dataset\n",
            "Training batch size: 64, Testing batch size: 256\n",
            "Pruning percentages: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
            "Number of runs per percentage: 10\n",
            "Epochs per run: 1\n",
            "Results will be shared with: lenishpandey@gmail.com\n",
            "\n",
            "Starting Parameter Mask Pruning experiment...\n",
            "\n",
            "Loading mnist dataset...\n",
            "\n",
            "Creating baseline model to calculate baseline CE...\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 226ms/step - accuracy: 0.0767 - categorical_crossentropy: 2.3030 - loss: 2.3030\n",
            "Baseline CE (CEo): 2.3030436038970947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-2d34758fad68>:564: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  summary_sheet.update('A1', [['Prune Percentage', 'Avg Test Accuracy', 'Std Test Accuracy',\n",
            "<ipython-input-2-2d34758fad68>:577: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  run_summary_sheet.update('A1', [run_headers])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Starting Run #1 =====\n",
            "\n",
            "Running experiment: mnist, parameter_mask, P% = 0, Run #1\n",
            "Using CE threshold ln(10) = 2.3026 for mnist\n",
            "Total trainable parameters: 7399616\n",
            "Parameters to prune: 0 (0%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-2d34758fad68>:279: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  self.worksheet.update('A1', [self.headers])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spreadsheet URL: https://docs.google.com/spreadsheets/d/1LCi-L80tAcu2qfyrfDGcJzzbEkmec912eQBfn_uvaKY\n",
            "CE dropped below threshold (2.3026) at batch 56 (CE = 2.2935)\n",
            "Error writing to Google Sheet: APIError: [429]: Quota exceeded for quota metric 'Write requests' and limit 'Write requests per minute' of service 'sheets.googleapis.com' for consumer 'project_number:522309567947'.\n",
            "Error writing to Google Sheet: APIError: [429]: Quota exceeded for quota metric 'Write requests' and limit 'Write requests per minute' of service 'sheets.googleapis.com' for consumer 'project_number:522309567947'.\n",
            "938/938 - 1952s - 2s/step - accuracy: 0.9853 - categorical_crossentropy: 0.0489 - loss: 0.0489 - val_accuracy: 0.9853 - val_categorical_crossentropy: 0.0489 - val_loss: 0.0489\n",
            "P0 Run #1 - Test Accuracy: 0.9853, IPA × 1000: 0.037549, CE below threshold\n",
            "\n",
            "Running experiment: mnist, parameter_mask, P% = 10, Run #1\n",
            "Using CE threshold ln(10) = 2.3026 for mnist\n",
            "Total trainable parameters: 7399616\n",
            "Parameters to prune: 739961 (10%)\n",
            "Spreadsheet URL: https://docs.google.com/spreadsheets/d/1LCi-L80tAcu2qfyrfDGcJzzbEkmec912eQBfn_uvaKY\n",
            "CE dropped below threshold (2.3026) at batch 23 (CE = 2.2955)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.layers import Input, BatchNormalization, ReLU, Conv2D, Dense, MaxPool2D, AvgPool2D, GlobalAvgPool2D, Concatenate\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10, cifar100\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Model\n",
        "import math\n",
        "import gspread\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuration\n",
        "BATCH_SIZE_TRAIN = 64\n",
        "BATCH_SIZE_TEST = 256\n",
        "LEARNING_RATE = 0.1\n",
        "PRUNE_PERCENTAGES = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # 0% to 100%\n",
        "NUMBER_OF_RUNS = 10  # Number of runs per pruning percentage\n",
        "EPOCHS_PER_RUN = 1   # Number of epochs per run\n",
        "EMAIL = \"lenishpandey@gmail.com\"           # Enter your gmail here for datasheet\n",
        "\n",
        "# Function to load and preprocess datasets\n",
        "def load_dataset(dataset_name):\n",
        "    if dataset_name == 'mnist':\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "        # Reshape to add channel dimension and resize to 32x32\n",
        "        x_train = np.pad(x_train, ((0, 0), (2, 2), (2, 2)), 'constant')\n",
        "        x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2)), 'constant')\n",
        "        x_train = np.expand_dims(x_train, axis=-1)\n",
        "        x_test = np.expand_dims(x_test, axis=-1)\n",
        "        # Repeat the channel to match the 3-channel input expected by DenseNet\n",
        "        x_train = np.repeat(x_train, 3, axis=-1)\n",
        "        x_test = np.repeat(x_test, 3, axis=-1)\n",
        "        num_classes = 10\n",
        "    elif dataset_name == 'fashion_mnist':\n",
        "        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "        x_train = np.pad(x_train, ((0, 0), (2, 2), (2, 2)), 'constant')\n",
        "        x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2)), 'constant')\n",
        "        x_train = np.expand_dims(x_train, axis=-1)\n",
        "        x_test = np.expand_dims(x_test, axis=-1)\n",
        "        x_train = np.repeat(x_train, 3, axis=-1)\n",
        "        x_test = np.repeat(x_test, 3, axis=-1)\n",
        "        num_classes = 10\n",
        "    elif dataset_name == 'cifar10':\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "        num_classes = 10\n",
        "    elif dataset_name == 'cifar100':\n",
        "        (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "        num_classes = 100\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "\n",
        "    # Normalize data\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "    # Convert class vectors to binary class matrices\n",
        "    y_train = to_categorical(y_train, num_classes)\n",
        "    y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test), num_classes\n",
        "\n",
        "# Define DenseNet-121 architecture\n",
        "def bn_rl_conv(x, filters, kernel_size):\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv2D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
        "    return x\n",
        "\n",
        "def dense_block(tensor, k, reps):\n",
        "    for _ in range(reps):\n",
        "        x = bn_rl_conv(tensor, filters=4 * k, kernel_size=1)\n",
        "        x = bn_rl_conv(x, filters=k, kernel_size=3)\n",
        "        tensor = Concatenate()([tensor, x])\n",
        "    return tensor\n",
        "\n",
        "def transition_layer(x, theta):\n",
        "    f = int(tf.keras.backend.int_shape(x)[-1] * theta)\n",
        "    x = bn_rl_conv(x, filters=f, kernel_size=1)\n",
        "    x = AvgPool2D(pool_size=2, strides=2, padding='same')(x)\n",
        "    return x\n",
        "\n",
        "def create_densenet121(input_shape, num_classes, k=32, theta=0.5):\n",
        "    # DenseNet-121 has repetitions [6, 12, 24, 16]\n",
        "    repetitions = [6, 12, 24, 16]\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(2 * k, 7, strides=2, padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "    for reps in repetitions:\n",
        "        x = dense_block(x, k, reps)\n",
        "        x = transition_layer(x, theta)\n",
        "\n",
        "    x = GlobalAvgPool2D()(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "# Parameter Mask Pruning function\n",
        "def parameter_mask_pruning(model, prune_percentage, seed=None):\n",
        "    \"\"\"\n",
        "    Randomly select a percentage of weights across the entire network and set them to zero.\n",
        "    These weights will remain fixed at zero throughout training.\n",
        "\n",
        "    Parameters:\n",
        "    model -- The Keras model to prune\n",
        "    prune_percentage -- Percentage of weights to prune (0-100)\n",
        "    seed -- Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    The pruned model with a custom weight mask\n",
        "    \"\"\"\n",
        "    # Set random seed if provided\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    # Get all trainable weights in the model\n",
        "    all_weights = []\n",
        "    all_shapes = []\n",
        "    all_layer_indices = []\n",
        "\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if isinstance(layer, (Conv2D, Dense)) and len(layer.weights) > 0:\n",
        "            # Only consider weight matrices, not biases\n",
        "            weight = layer.get_weights()[0]\n",
        "            all_weights.append(weight)\n",
        "            all_shapes.append(weight.shape)\n",
        "            all_layer_indices.append(i)\n",
        "\n",
        "    # Count total parameters\n",
        "    total_params = sum(w.size for w in all_weights)\n",
        "    num_to_prune = int(total_params * prune_percentage / 100)\n",
        "\n",
        "    print(f\"Total trainable parameters: {total_params}\")\n",
        "    print(f\"Parameters to prune: {num_to_prune} ({prune_percentage}%)\")\n",
        "\n",
        "    # Create masks for each layer (initially all ones)\n",
        "    masks = [np.ones_like(w) for w in all_weights]\n",
        "\n",
        "    # If pruning percentage is not 0, create masks and apply them\n",
        "    if prune_percentage > 0:\n",
        "        # Randomly select indices to prune across all parameters\n",
        "        flat_indices = np.random.choice(total_params, size=num_to_prune, replace=False)\n",
        "\n",
        "        # Map flat indices back to layer, row, col indices\n",
        "        params_so_far = 0\n",
        "        for i, weight in enumerate(all_weights):\n",
        "            size = weight.size\n",
        "            # Get indices that fall within this layer\n",
        "            indices_in_layer = flat_indices[(flat_indices >= params_so_far) &\n",
        "                                            (flat_indices < params_so_far + size)] - params_so_far\n",
        "\n",
        "            # Flatten the mask, set the selected indices to zero, then reshape back\n",
        "            flat_mask = masks[i].flatten()\n",
        "            flat_mask[indices_in_layer] = 0\n",
        "            masks[i] = flat_mask.reshape(all_shapes[i])\n",
        "\n",
        "            params_so_far += size\n",
        "\n",
        "        # Apply masks to each layer's weights\n",
        "        for i, layer_idx in enumerate(all_layer_indices):\n",
        "            layer = model.layers[layer_idx]\n",
        "            weights = layer.get_weights()\n",
        "            weights[0] = weights[0] * masks[i]  # Apply mask to weights\n",
        "            layer.set_weights(weights)\n",
        "\n",
        "    # Create a custom callback to enforce the mask during training\n",
        "    class MaskWeightsCallback(tf.keras.callbacks.Callback):\n",
        "        def __init__(self, masks, layer_indices):\n",
        "            self.masks = masks\n",
        "            self.layer_indices = layer_indices\n",
        "\n",
        "        def on_batch_end(self, batch, logs=None):\n",
        "            # Apply masks after each batch update\n",
        "            for i, layer_idx in enumerate(self.layer_indices):\n",
        "                layer = self.model.layers[layer_idx]\n",
        "                weights = layer.get_weights()\n",
        "                weights[0] = weights[0] * self.masks[i]\n",
        "                layer.set_weights(weights)\n",
        "\n",
        "    # Attach the mask callback to the model for later use\n",
        "    model.mask_callback = MaskWeightsCallback(masks, all_layer_indices)\n",
        "\n",
        "    # Reset random seed\n",
        "    if seed is not None:\n",
        "        np.random.seed(None)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Google Sheets Logger class\n",
        "class GoogleSheetsLogger(Callback):\n",
        "    def __init__(self, test_data, batch_size, ce_threshold, spreadsheet=None, sheet_name=None, baseline_ce=None):\n",
        "        \"\"\"\n",
        "        Initialize the Google Sheets Logger\n",
        "\n",
        "        Parameters:\n",
        "        test_data -- tuple of (x_test, y_test)\n",
        "        batch_size -- batch size for evaluation\n",
        "        ce_threshold -- Threshold for CE (ln(10) or ln(100) depending on dataset)\n",
        "        spreadsheet -- existing Google spreadsheet object to use\n",
        "        sheet_name -- name of the worksheet to create/use within the spreadsheet\n",
        "        baseline_ce -- baseline cross-entropy from unpruned model (for IPA calculation)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.test_x, self.test_y = test_data\n",
        "        self.batch_size = batch_size\n",
        "        self.total_batches = 0  # Using total_batches for continuous numbering across epochs\n",
        "        self.epoch = 0\n",
        "        self.baseline_ce = baseline_ce\n",
        "        self.ce_threshold = ce_threshold  # Store the CE threshold (ln(10) or ln(100))\n",
        "        self.ipa_start_batch = None  # Track when CE first goes below threshold\n",
        "        self.spreadsheet = spreadsheet\n",
        "\n",
        "        # Use provided sheet name or generate a default\n",
        "        if sheet_name is None:\n",
        "            sheet_name = f\"Training_Log_{int(time.time())}\"\n",
        "        self.sheet_name = sheet_name\n",
        "\n",
        "        # Column headers for the sheet\n",
        "        self.headers = [\n",
        "            'Epoch', 'Batch Number',\n",
        "            'Train Accur', 'Train CE', 'Test Accur', 'Test CE',\n",
        "            '|CE(b)-CEo|', 'IPA'\n",
        "        ]\n",
        "\n",
        "        # Authenticate and setup Google Sheets\n",
        "        self._setup_google_sheets()\n",
        "\n",
        "    def _setup_google_sheets(self):\n",
        "        \"\"\"Setup authentication and create the Google Sheet\"\"\"\n",
        "        # Authenticate to Google if not already done\n",
        "        if self.spreadsheet is None:\n",
        "            auth.authenticate_user()\n",
        "\n",
        "            # Get credentials (using google-auth instead of oauth2client)\n",
        "            creds, _ = default()\n",
        "\n",
        "            # Create a gspread client\n",
        "            self.gc = gspread.authorize(creds)\n",
        "\n",
        "            try:\n",
        "                # Try to open existing spreadsheet\n",
        "                self.sheet = self.gc.open(\"DenseNet121_Pruning_Experiments\")\n",
        "                print(f\"Using existing spreadsheet: DenseNet121_Pruning_Experiments\")\n",
        "            except gspread.exceptions.SpreadsheetNotFound:\n",
        "                # Create new spreadsheet if it doesn't exist\n",
        "                self.sheet = self.gc.create(\"DenseNet121_Pruning_Experiments\")\n",
        "                print(f\"Created new spreadsheet: DenseNet121_Pruning_Experiments\")\n",
        "\n",
        "                # Share the spreadsheet with the specified email if provided\n",
        "                if EMAIL:\n",
        "                    self.sheet.share(EMAIL, perm_type='user', role='writer')\n",
        "                    print(f\"Shared spreadsheet with: {EMAIL}\")\n",
        "\n",
        "            # Store for future use\n",
        "            self.spreadsheet = self.sheet\n",
        "        else:\n",
        "            # Use the provided spreadsheet\n",
        "            self.sheet = self.spreadsheet\n",
        "\n",
        "        # Get or create the worksheet for this pruning percentage\n",
        "        try:\n",
        "            self.worksheet = self.sheet.worksheet(self.sheet_name)\n",
        "            # Clear existing content\n",
        "            self.worksheet.clear()\n",
        "        except gspread.exceptions.WorksheetNotFound:\n",
        "            self.worksheet = self.sheet.add_worksheet(title=self.sheet_name, rows=1000, cols=len(self.headers))\n",
        "\n",
        "        # Add headers to the worksheet\n",
        "        self.worksheet.update('A1', [self.headers])\n",
        "\n",
        "        # Print the URL to access the spreadsheet\n",
        "        print(f\"Spreadsheet URL: {self.sheet.url}\")\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        # If baseline CE is not provided, calculate it\n",
        "        if self.baseline_ce is None:\n",
        "            print(\"Calculating baseline CE...\")\n",
        "            _, _, self.baseline_ce = self.model.evaluate(\n",
        "                self.test_x, self.test_y,\n",
        "                batch_size=self.batch_size,\n",
        "                verbose=1\n",
        "            )\n",
        "            print(f\"Baseline CE (CEo): {self.baseline_ce}\")\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.epoch = epoch + 1\n",
        "        # No batch number reset - using total_batches for continuous counting\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        self.total_batches += 1\n",
        "\n",
        "        # Calculate metrics on test data (silently)\n",
        "        _, test_acc, test_ce = self.model.evaluate(\n",
        "            self.test_x, self.test_y,\n",
        "            batch_size=self.batch_size,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Calculate CE difference\n",
        "        ce_diff = abs(test_ce - self.baseline_ce)\n",
        "\n",
        "        # Check if CE is below threshold and update ipa_start_batch if first time\n",
        "        if self.ipa_start_batch is None and test_ce <= self.ce_threshold:\n",
        "            self.ipa_start_batch = self.total_batches\n",
        "            print(f\"CE dropped below threshold ({self.ce_threshold:.4f}) at batch {self.ipa_start_batch} (CE = {test_ce:.4f})\")\n",
        "\n",
        "        # Calculate IPA\n",
        "        # If CE is above threshold, IPA is None\n",
        "        # Otherwise, use the current batch number (not adjusted)\n",
        "        if test_ce <= self.ce_threshold:\n",
        "            ipa = ce_diff / (BATCH_SIZE_TRAIN * self.total_batches) if self.total_batches > 0 else 0\n",
        "            ipa_value = ipa\n",
        "        else:\n",
        "            ipa_value = \"N/A\"\n",
        "\n",
        "        # Prepare row to append\n",
        "        row = [\n",
        "            self.epoch,\n",
        "            self.total_batches,\n",
        "            logs.get('accuracy', 0),\n",
        "            logs.get('loss', 0),  # Train CE\n",
        "            test_acc,\n",
        "            test_ce,\n",
        "            ce_diff,\n",
        "            ipa_value\n",
        "        ]\n",
        "\n",
        "        # Append row to Google Sheet\n",
        "        try:\n",
        "            self.worksheet.append_row(row)\n",
        "        except Exception as e:\n",
        "            print(f\"Error writing to Google Sheet: {e}\")\n",
        "\n",
        "    def get_final_metrics(self):\n",
        "        \"\"\"Return the final IPA and test accuracy\"\"\"\n",
        "        # Try to get the latest values from the Google Sheet\n",
        "        try:\n",
        "            values = self.worksheet.get_all_values()\n",
        "            if len(values) > 1:  # If we have data (header + at least one row)\n",
        "                last_row = values[-1]\n",
        "\n",
        "                # Check if the IPA is a number or N/A\n",
        "                ipa_value = last_row[7]  # Index 7 is IPA\n",
        "                if ipa_value != \"N/A\":\n",
        "                    return {\n",
        "                        'Final Test Accuracy': float(last_row[4]),  # Index 4 is Test Accur\n",
        "                        'IPA': float(ipa_value) * 1000,  # Scale by 1000\n",
        "                        'CE Below Threshold': True\n",
        "                    }\n",
        "                else:\n",
        "                    return {\n",
        "                        'Final Test Accuracy': float(last_row[4]),  # Index 4 is Test Accur\n",
        "                        'IPA': 0,  # IPA is 0 if N/A\n",
        "                        'CE Below Threshold': False\n",
        "                    }\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting final metrics from Google Sheet: {e}\")\n",
        "\n",
        "        # Fall back to calculating final metrics\n",
        "        _, test_acc, test_ce = self.model.evaluate(\n",
        "            self.test_x, self.test_y,\n",
        "            batch_size=self.batch_size,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        ce_diff = abs(test_ce - self.baseline_ce)\n",
        "\n",
        "        # Check if CE is below threshold\n",
        "        if test_ce <= self.ce_threshold:\n",
        "            ipa = ce_diff / (BATCH_SIZE_TRAIN * self.total_batches) if self.total_batches > 0 else 0\n",
        "            return {\n",
        "                'Final Test Accuracy': test_acc,\n",
        "                'IPA': ipa * 1000,  # Scale by 1000\n",
        "                'CE Below Threshold': True\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'Final Test Accuracy': test_acc,\n",
        "                'IPA': 0,  # IPA is 0 if CE is above threshold\n",
        "                'CE Below Threshold': False\n",
        "            }\n",
        "\n",
        "# Function to run a single experiment with a specific pruning percentage and seed\n",
        "def run_single_experiment(dataset_name, pruning_method, prune_percentage, run_number, baseline_ce, spreadsheet, x_train, y_train, x_test, y_test, num_classes):\n",
        "    \"\"\"\n",
        "    Run a single experiment with a specific pruning percentage and seed.\n",
        "\n",
        "    Parameters:\n",
        "    dataset_name -- Name of the dataset\n",
        "    pruning_method -- Pruning method to use\n",
        "    prune_percentage -- Percentage to prune\n",
        "    run_number -- Run number (for setting seed)\n",
        "    baseline_ce -- Baseline cross-entropy\n",
        "    spreadsheet -- Google Sheets spreadsheet object\n",
        "    x_train, y_train, x_test, y_test -- Training and test data\n",
        "    num_classes -- Number of classes\n",
        "\n",
        "    Returns:\n",
        "    Dictionary with results\n",
        "    \"\"\"\n",
        "    print(f\"\\nRunning experiment: {dataset_name}, {pruning_method}, P% = {prune_percentage}, Run #{run_number}\")\n",
        "\n",
        "    # Use run number as seed for reproducibility\n",
        "    seed = 42 + run_number\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "    # Define CE threshold based on dataset\n",
        "    if dataset_name == 'cifar100':\n",
        "        ce_threshold = math.log(100)  # ln(100) for CIFAR-100\n",
        "        print(f\"Using CE threshold ln(100) = {ce_threshold:.4f} for CIFAR-100\")\n",
        "    else:\n",
        "        ce_threshold = math.log(10)  # ln(10) for other datasets\n",
        "        print(f\"Using CE threshold ln(10) = {ce_threshold:.4f} for {dataset_name}\")\n",
        "\n",
        "    # Create model\n",
        "    model = create_densenet121(input_shape=x_train.shape[1:], num_classes=num_classes)\n",
        "\n",
        "    # Apply parameter mask pruning with seed\n",
        "    model = parameter_mask_pruning(model, prune_percentage, seed=seed)\n",
        "\n",
        "    # Store the mask callback for use during training\n",
        "    mask_callback = model.mask_callback\n",
        "\n",
        "    # Compile model\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', 'categorical_crossentropy']\n",
        "    )\n",
        "\n",
        "    # Setup Google Sheets logger\n",
        "    sheet_name = f\"Run{run_number}_P{prune_percentage}\"\n",
        "    google_sheet_logger = GoogleSheetsLogger(\n",
        "        test_data=(x_test, y_test),\n",
        "        batch_size=BATCH_SIZE_TEST,\n",
        "        ce_threshold=ce_threshold,\n",
        "        spreadsheet=spreadsheet,\n",
        "        sheet_name=sheet_name,\n",
        "        baseline_ce=baseline_ce\n",
        "    )\n",
        "\n",
        "    # Setup callbacks\n",
        "    callbacks = [google_sheet_logger, mask_callback]\n",
        "\n",
        "    # Train for specified number of epochs\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        batch_size=BATCH_SIZE_TRAIN,\n",
        "        validation_batch_size=BATCH_SIZE_TEST,\n",
        "        epochs=EPOCHS_PER_RUN,\n",
        "        validation_data=(x_test, y_test),\n",
        "        callbacks=callbacks,\n",
        "        verbose=2  # Less verbose output\n",
        "    )\n",
        "\n",
        "    # Get final metrics\n",
        "    final_metrics = google_sheet_logger.get_final_metrics()\n",
        "\n",
        "    # Clear memory\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    result = {\n",
        "        'Pruning Percentage': prune_percentage,\n",
        "        'Run Number': run_number,\n",
        "        'Final Test Accuracy': final_metrics['Final Test Accuracy'],\n",
        "        'IPA': final_metrics['IPA'],\n",
        "        'CE Below Threshold': final_metrics['CE Below Threshold']\n",
        "    }\n",
        "\n",
        "    threshold_status = \"CE below threshold\" if final_metrics['CE Below Threshold'] else \"CE above threshold (IPA set to 0)\"\n",
        "    print(f\"P{prune_percentage} Run #{run_number} - Test Accuracy: {result['Final Test Accuracy']:.4f}, IPA × 1000: {result['IPA']:.6f}, {threshold_status}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "# Function to run experiment for multiple pruning percentages with multiple runs\n",
        "def run_multi_experiment(dataset_name, pruning_method):\n",
        "    \"\"\"\n",
        "    Run experiments for multiple pruning percentages with multiple runs each.\n",
        "    Modified to complete one full run across all pruning percentages before starting the next run.\n",
        "\n",
        "    Parameters:\n",
        "    dataset_name -- Name of the dataset\n",
        "    pruning_method -- Pruning method to use\n",
        "\n",
        "    Returns:\n",
        "    DataFrame with summary results\n",
        "    \"\"\"\n",
        "    # Create output directory with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_dir = f\"results_{dataset_name}_{pruning_method}_{timestamp}\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Create subdirectory for plots\n",
        "    plots_dir = os.path.join(output_dir, \"plots\")\n",
        "    os.makedirs(plots_dir, exist_ok=True)\n",
        "\n",
        "    # Create a CSV file to store results for all runs\n",
        "    all_runs_file = os.path.join(output_dir, \"all_runs_results.csv\")\n",
        "    with open(all_runs_file, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Run Number', 'Pruning Percentage', 'Final Test Accuracy', 'IPA', 'CE Below Threshold'])\n",
        "\n",
        "    # Load dataset once\n",
        "    print(f\"\\nLoading {dataset_name} dataset...\")\n",
        "    (x_train, y_train), (x_test, y_test), num_classes = load_dataset(dataset_name)\n",
        "\n",
        "    # Calculate baseline CE from unpruned model once\n",
        "    print(\"\\nCreating baseline model to calculate baseline CE...\")\n",
        "    baseline_model = create_densenet121(input_shape=x_train.shape[1:], num_classes=num_classes)\n",
        "\n",
        "    # Compile baseline model\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
        "    baseline_model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', 'categorical_crossentropy']\n",
        "    )\n",
        "\n",
        "    # Calculate baseline CE\n",
        "    _, _, baseline_ce = baseline_model.evaluate(\n",
        "        x_test, y_test,\n",
        "        batch_size=BATCH_SIZE_TEST,\n",
        "        verbose=1\n",
        "    )\n",
        "    print(f\"Baseline CE (CEo): {baseline_ce}\")\n",
        "\n",
        "    # Clear memory\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # Setup Google Sheets\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "\n",
        "    try:\n",
        "        # Try to open existing spreadsheet\n",
        "        sheet = gc.open(\"DenseNet121_Pruning_Experiments\")\n",
        "    except gspread.exceptions.SpreadsheetNotFound:\n",
        "        # Create new spreadsheet if it doesn't exist\n",
        "        sheet = gc.create(\"DenseNet121_Pruning_Experiments\")\n",
        "        if EMAIL:\n",
        "            sheet.share(EMAIL, perm_type='user', role='writer')\n",
        "\n",
        "    # Create or update summary worksheet\n",
        "    summary_name = f\"{dataset_name}_{pruning_method}_summary_{timestamp}\"\n",
        "    try:\n",
        "        summary_sheet = sheet.worksheet(summary_name)\n",
        "        summary_sheet.clear()\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        summary_sheet = sheet.add_worksheet(title=summary_name, rows=100, cols=6)\n",
        "\n",
        "    # Add headers to summary sheet\n",
        "    summary_sheet.update('A1', [['Prune Percentage', 'Avg Test Accuracy', 'Std Test Accuracy',\n",
        "                               'Avg IPA × 1000', 'Std IPA × 1000', 'CE Below Threshold Count']])\n",
        "\n",
        "    # Create run-wise summary worksheet\n",
        "    run_summary_name = f\"{dataset_name}_{pruning_method}_runs_{timestamp}\"\n",
        "    try:\n",
        "        run_summary_sheet = sheet.worksheet(run_summary_name)\n",
        "        run_summary_sheet.clear()\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        run_summary_sheet = sheet.add_worksheet(title=run_summary_name, rows=100, cols=len(PRUNE_PERCENTAGES) + 1)\n",
        "\n",
        "    # Add headers to run summary sheet - Run Number and all pruning percentages\n",
        "    run_headers = ['Run Number'] + [f'P{p}%' for p in PRUNE_PERCENTAGES]\n",
        "    run_summary_sheet.update('A1', [run_headers])\n",
        "\n",
        "    # Store all results\n",
        "    all_results = []\n",
        "\n",
        "    # Dictionary to group results by pruning percentage\n",
        "    results_by_percentage = {p: [] for p in PRUNE_PERCENTAGES}\n",
        "\n",
        "    # Loop through runs first\n",
        "    for run_number in range(1, NUMBER_OF_RUNS + 1):\n",
        "        print(f\"\\n===== Starting Run #{run_number} =====\")\n",
        "\n",
        "        # Store IPA values for all pruning percentages in this run\n",
        "        run_ipa_values = []\n",
        "\n",
        "        # Loop through pruning percentages in each run\n",
        "        for prune_percentage in PRUNE_PERCENTAGES:\n",
        "            # Run a single experiment\n",
        "            result = run_single_experiment(\n",
        "                dataset_name=dataset_name,\n",
        "                pruning_method=pruning_method,\n",
        "                prune_percentage=prune_percentage,\n",
        "                run_number=run_number,\n",
        "                baseline_ce=baseline_ce,\n",
        "                spreadsheet=sheet,\n",
        "                x_train=x_train,\n",
        "                y_train=y_train,\n",
        "                x_test=x_test,\n",
        "                y_test=y_test,\n",
        "                num_classes=num_classes\n",
        "            )\n",
        "\n",
        "            # Add to overall results\n",
        "            all_results.append(result)\n",
        "\n",
        "            # Add to results by percentage\n",
        "            results_by_percentage[prune_percentage].append(result)\n",
        "\n",
        "            # Add IPA value for this pruning percentage\n",
        "            run_ipa_values.append(result['IPA'])\n",
        "\n",
        "            # Append to CSV file\n",
        "            with open(all_runs_file, 'a', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\n",
        "                    result['Run Number'],\n",
        "                    result['Pruning Percentage'],\n",
        "                    result['Final Test Accuracy'],\n",
        "                    result['IPA'],\n",
        "                    result['CE Below Threshold']\n",
        "                ])\n",
        "\n",
        "        # Update run summary sheet with IPAs from this run\n",
        "        run_summary_sheet.append_row([run_number] + run_ipa_values)\n",
        "\n",
        "        print(f\"\\nCompleted Run #{run_number} across all pruning percentages\")\n",
        "\n",
        "    # Calculate and store summary statistics for each pruning percentage\n",
        "    summary_results = []\n",
        "\n",
        "    for prune_percentage, results in results_by_percentage.items():\n",
        "        accuracies = [r['Final Test Accuracy'] for r in results]\n",
        "        ipas = [r['IPA'] for r in results]\n",
        "        threshold_count = sum(1 for r in results if r['CE Below Threshold'])\n",
        "\n",
        "        avg_accuracy = np.mean(accuracies)\n",
        "        std_accuracy = np.std(accuracies)\n",
        "        avg_ipa = np.mean(ipas)\n",
        "        std_ipa = np.std(ipas)\n",
        "\n",
        "        # Add to summary results\n",
        "        summary_results.append({\n",
        "            'Pruning Percentage': prune_percentage,\n",
        "            'Avg Test Accuracy': avg_accuracy,\n",
        "            'Std Test Accuracy': std_accuracy,\n",
        "            'Avg IPA': avg_ipa,\n",
        "            'Std IPA': std_ipa,\n",
        "            'CE Below Threshold Count': threshold_count\n",
        "        })\n",
        "\n",
        "        # Add to summary sheet\n",
        "        summary_sheet.append_row([\n",
        "            prune_percentage,\n",
        "            avg_accuracy,\n",
        "            std_accuracy,\n",
        "            avg_ipa,\n",
        "            std_ipa,\n",
        "            threshold_count\n",
        "        ])\n",
        "\n",
        "        print(f\"\\nSummary for P{prune_percentage}:\")\n",
        "        print(f\"  Avg Test Accuracy: {avg_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
        "        print(f\"  Avg IPA × 1000: {avg_ipa:.6f} ± {std_ipa:.6f}\")\n",
        "        print(f\"  Runs with CE below threshold: {threshold_count}/{NUMBER_OF_RUNS}\")\n",
        "\n",
        "    # Create summary DataFrame\n",
        "    summary_df = pd.DataFrame(summary_results)\n",
        "\n",
        "    # Save summary to CSV\n",
        "    summary_df.to_csv(os.path.join(output_dir, \"summary_results.csv\"), index=False)\n",
        "\n",
        "    # Plot IPA vs Pruning Percentage with error bars\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.errorbar(\n",
        "        summary_df['Pruning Percentage'],\n",
        "        summary_df['Avg IPA'],\n",
        "        yerr=summary_df['Std IPA'],\n",
        "        fmt='o-',\n",
        "        capsize=5,\n",
        "        elinewidth=2,\n",
        "        markeredgewidth=2\n",
        "    )\n",
        "    plt.xlabel('Pruning Percentage (P%)')\n",
        "    plt.ylabel('Average IPA × 1000')\n",
        "    plt.title(f'IPA vs P% for {dataset_name} using {pruning_method} pruning')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(plots_dir, \"avg_ipa_vs_prune.png\"))\n",
        "\n",
        "    # Plot Accuracy vs Pruning Percentage with error bars\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.errorbar(\n",
        "        summary_df['Pruning Percentage'],\n",
        "        summary_df['Avg Test Accuracy'],\n",
        "        yerr=summary_df['Std Test Accuracy'],\n",
        "        fmt='o-',\n",
        "        capsize=5,\n",
        "        eelinewidth=2,\n",
        "        markeredgewidth=2\n",
        "    )\n",
        "    plt.xlabel('Pruning Percentage (P%)')\n",
        "    plt.ylabel('Average Test Accuracy')\n",
        "    plt.title(f'Accuracy vs P% for {dataset_name} using {pruning_method} pruning')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(plots_dir, \"avg_accuracy_vs_prune.png\"))\n",
        "\n",
        "    # Plot CE Below Threshold Count vs Pruning Percentage\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(\n",
        "        summary_df['Pruning Percentage'],\n",
        "        summary_df['CE Below Threshold Count'],\n",
        "        alpha=0.7\n",
        "    )\n",
        "    plt.xlabel('Pruning Percentage (P%)')\n",
        "    plt.ylabel('Count of Runs with CE Below Threshold')\n",
        "    plt.title(f'Convergence Rate vs P% for {dataset_name} using {pruning_method} pruning')\n",
        "    plt.grid(True, axis='y')\n",
        "    plt.savefig(os.path.join(plots_dir, \"convergence_rate_vs_prune.png\"))\n",
        "\n",
        "    # Create heatmap of IPA values across runs and pruning percentages\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Extract data for heatmap\n",
        "    heatmap_data = np.zeros((NUMBER_OF_RUNS, len(PRUNE_PERCENTAGES)))\n",
        "    for i, prune_percentage in enumerate(PRUNE_PERCENTAGES):\n",
        "        for j, result in enumerate(results_by_percentage[prune_percentage]):\n",
        "            heatmap_data[j, i] = result['IPA']\n",
        "\n",
        "    # Create heatmap\n",
        "    plt.imshow(heatmap_data, cmap='viridis', aspect='auto')\n",
        "    plt.colorbar(label='IPA × 1000')\n",
        "\n",
        "    # Set ticks and labels\n",
        "    plt.xticks(np.arange(len(PRUNE_PERCENTAGES)), [f'{p}%' for p in PRUNE_PERCENTAGES])\n",
        "    plt.yticks(np.arange(NUMBER_OF_RUNS), [f'Run {i+1}' for i in range(NUMBER_OF_RUNS)])\n",
        "\n",
        "    plt.xlabel('Pruning Percentage')\n",
        "    plt.ylabel('Run Number')\n",
        "    plt.title(f'IPA Heatmap for {dataset_name} using {pruning_method} pruning')\n",
        "    plt.savefig(os.path.join(plots_dir, \"ipa_heatmap.png\"))\n",
        "\n",
        "    # Close all plots\n",
        "    plt.close('all')\n",
        "\n",
        "    return summary_df, output_dir\n",
        "\n",
        "# Main execution - Select dataset and run experiment\n",
        "def main():\n",
        "    # Options for datasets\n",
        "    dataset_options = {\n",
        "        '1': 'mnist',\n",
        "        '2': 'fashion_mnist',\n",
        "        '3': 'cifar10',\n",
        "        '4': 'cifar100'\n",
        "    }\n",
        "\n",
        "    # Print options\n",
        "    print(\"Available datasets:\")\n",
        "    for key, value in dataset_options.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    # Get user input or use default\n",
        "    try:\n",
        "        dataset_choice = input(\"Select dataset (1-4) or press Enter for MNIST: \")\n",
        "        if dataset_choice == \"\":\n",
        "            dataset_choice = \"1\"  # Default to MNIST\n",
        "\n",
        "        # Validate input\n",
        "        if dataset_choice not in dataset_options:\n",
        "            print(f\"Invalid choice '{dataset_choice}'. Using default (MNIST).\")\n",
        "            dataset_choice = \"1\"\n",
        "\n",
        "        dataset = dataset_options[dataset_choice]\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}. Using default dataset (MNIST).\")\n",
        "        dataset = 'mnist'\n",
        "\n",
        "    # Use parameter_mask pruning\n",
        "    pruning_method = 'parameter_mask'\n",
        "\n",
        "    print(f\"Running DenseNet-121 pruning experiments on {dataset} dataset\")\n",
        "    print(f\"Training batch size: {BATCH_SIZE_TRAIN}, Testing batch size: {BATCH_SIZE_TEST}\")\n",
        "    print(f\"Pruning percentages: {PRUNE_PERCENTAGES}\")\n",
        "    print(f\"Number of runs per percentage: {NUMBER_OF_RUNS}\")\n",
        "    print(f\"Epochs per run: {EPOCHS_PER_RUN}\")\n",
        "    if EMAIL:\n",
        "        print(f\"Results will be shared with: {EMAIL}\")\n",
        "\n",
        "    # Configure TensorFlow to be less verbose\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "    # Run parameter mask pruning experiment\n",
        "    print(\"\\nStarting Parameter Mask Pruning experiment...\")\n",
        "    summary_df, output_dir = run_multi_experiment(dataset, pruning_method)\n",
        "\n",
        "    print(\"\\nExperiment complete!\")\n",
        "    print(f\"Results saved to {output_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqdfZlG6wb14"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}