{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lenishu/IPA_using_Densenet/blob/main/Version_1_Pruning_Densenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRHbmKz5wNkV",
        "outputId": "e6818955-c0ae-4f6b-bcbf-132355fbfcf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running DenseNet-121 pruning experiments on mnist dataset\n",
            "Training batch size: 5000, Testing batch size: 5000\n",
            "Pruning percentages: [90]\n",
            "Results will be shared with: lenishpandey02@gmail.com\n",
            "\n",
            "Starting Parameter Mask Pruning experiment...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "\n",
            "Creating baseline model to calculate baseline CE...\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 259ms/step - accuracy: 0.0908 - categorical_crossentropy: 2.3023 - loss: 2.3023\n",
            "Baseline CE (CEo): 2.302340030670166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-ff7e34b4fd8a>:433: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  summary_sheet.update('A1', [['Prune Percentage', 'Final Test Accuracy', 'IPA × 1000']])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running experiment: mnist, parameter_mask, P% = 90\n",
            "Total parameters before pruning: 7,577,674\n",
            "Total trainable parameters: 7399616\n",
            "Parameters to prune: 6659654 (90%)\n",
            "Applied parameter mask pruning: P% = 90%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-ff7e34b4fd8a>:266: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  self.worksheet.update('A1', [self.headers])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spreadsheet URL: https://docs.google.com/spreadsheets/d/1LCi-L80tAcu2qfyrfDGcJzzbEkmec912eQBfn_uvaKY\n",
            "Training with batch size: 5000\n",
            "Validation batch size: 5000\n",
            "Logging to Google Sheet: P90\n",
            "Epoch 1/5\n",
            "Started IPA calculation at batch 1 (CE = 2.3026)\n",
            "12/12 - 224s - 19s/step - accuracy: 0.1135 - categorical_crossentropy: 2.3022 - loss: 2.3022 - val_accuracy: 0.1135 - val_categorical_crossentropy: 2.3022 - val_loss: 2.3022\n",
            "Epoch 2/5\n",
            "12/12 - 34s - 3s/step - accuracy: 0.1135 - categorical_crossentropy: 2.3024 - loss: 2.3024 - val_accuracy: 0.1135 - val_categorical_crossentropy: 2.3024 - val_loss: 2.3024\n",
            "Epoch 3/5\n",
            "12/12 - 37s - 3s/step - accuracy: 0.1009 - categorical_crossentropy: 2.3034 - loss: 2.3034 - val_accuracy: 0.0982 - val_categorical_crossentropy: 2.3033 - val_loss: 2.3033\n",
            "Epoch 4/5\n",
            "12/12 - 40s - 3s/step - accuracy: 0.1009 - categorical_crossentropy: 2.3047 - loss: 2.3047 - val_accuracy: 0.1009 - val_categorical_crossentropy: 2.3046 - val_loss: 2.3046\n",
            "Epoch 5/5\n",
            "12/12 - 40s - 3s/step - accuracy: 0.1009 - categorical_crossentropy: 2.3054 - loss: 2.3054 - val_accuracy: 0.1009 - val_categorical_crossentropy: 2.3054 - val_loss: 2.3054\n",
            "Pruning 90% - Test Accuracy: 0.1009, IPA × 1000: 0.000010\n",
            "\n",
            "Experiment complete!\n",
            "Results saved to results_mnist_parameter_mask/experiment_results.csv\n",
            "Plots saved to results_mnist_parameter_mask/\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.layers import Input, BatchNormalization, ReLU, Conv2D, Dense, MaxPool2D, AvgPool2D, GlobalAvgPool2D, Concatenate\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10, cifar100\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Model\n",
        "import math\n",
        "import gspread\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import time\n",
        "\n",
        "# Configuration\n",
        "BATCH_SIZE_TRAIN = 5000\n",
        "BATCH_SIZE_TEST = 5000\n",
        "LEARNING_RATE = 0.1\n",
        "PRUNE_PERCENTAGES = [90]\n",
        "EMAIL = \"\"  # Enter your gmail here for datasheet\n",
        "\n",
        "# Function to load and preprocess datasets\n",
        "def load_dataset(dataset_name):\n",
        "    if dataset_name == 'mnist':\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "        # Reshape to add channel dimension and resize to 32x32\n",
        "        x_train = np.pad(x_train, ((0, 0), (2, 2), (2, 2)), 'constant')\n",
        "        x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2)), 'constant')\n",
        "        x_train = np.expand_dims(x_train, axis=-1)\n",
        "        x_test = np.expand_dims(x_test, axis=-1)\n",
        "        # Repeat the channel to match the 3-channel input expected by DenseNet\n",
        "        x_train = np.repeat(x_train, 3, axis=-1)\n",
        "        x_test = np.repeat(x_test, 3, axis=-1)\n",
        "        num_classes = 10\n",
        "    elif dataset_name == 'fashion_mnist':\n",
        "        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "        x_train = np.pad(x_train, ((0, 0), (2, 2), (2, 2)), 'constant')\n",
        "        x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2)), 'constant')\n",
        "        x_train = np.expand_dims(x_train, axis=-1)\n",
        "        x_test = np.expand_dims(x_test, axis=-1)\n",
        "        x_train = np.repeat(x_train, 3, axis=-1)\n",
        "        x_test = np.repeat(x_test, 3, axis=-1)\n",
        "        num_classes = 10\n",
        "    elif dataset_name == 'cifar10':\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "        num_classes = 10\n",
        "    elif dataset_name == 'cifar100':\n",
        "        (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "        num_classes = 100\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "\n",
        "    # Normalize data\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "    # Convert class vectors to binary class matrices\n",
        "    y_train = to_categorical(y_train, num_classes)\n",
        "    y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test), num_classes\n",
        "\n",
        "# Define DenseNet-121 architecture\n",
        "def bn_rl_conv(x, filters, kernel_size):\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv2D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
        "    return x\n",
        "\n",
        "def dense_block(tensor, k, reps):\n",
        "    for _ in range(reps):\n",
        "        x = bn_rl_conv(tensor, filters=4 * k, kernel_size=1)\n",
        "        x = bn_rl_conv(x, filters=k, kernel_size=3)\n",
        "        tensor = Concatenate()([tensor, x])\n",
        "    return tensor\n",
        "\n",
        "def transition_layer(x, theta):\n",
        "    f = int(tf.keras.backend.int_shape(x)[-1] * theta)\n",
        "    x = bn_rl_conv(x, filters=f, kernel_size=1)\n",
        "    x = AvgPool2D(pool_size=2, strides=2, padding='same')(x)\n",
        "    return x\n",
        "\n",
        "def create_densenet121(input_shape, num_classes, k=32, theta=0.5):\n",
        "    # DenseNet-121 has repetitions [6, 12, 24, 16]\n",
        "    repetitions = [6, 12, 24, 16]\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(2 * k, 7, strides=2, padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "    for reps in repetitions:\n",
        "        x = dense_block(x, k, reps)\n",
        "        x = transition_layer(x, theta)\n",
        "\n",
        "    x = GlobalAvgPool2D()(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "# Parameter Mask Pruning function\n",
        "def parameter_mask_pruning(model, prune_percentage):\n",
        "    \"\"\"\n",
        "    Randomly select a percentage of weights across the entire network and set them to zero.\n",
        "    These weights will remain fixed at zero throughout training.\n",
        "\n",
        "    Parameters:\n",
        "    model -- The Keras model to prune\n",
        "    prune_percentage -- Percentage of weights to prune (0-100)\n",
        "\n",
        "    Returns:\n",
        "    The pruned model with a custom weight mask\n",
        "    \"\"\"\n",
        "    # Get all trainable weights in the model\n",
        "    all_weights = []\n",
        "    all_shapes = []\n",
        "    all_layer_indices = []\n",
        "\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if isinstance(layer, (Conv2D, Dense)) and len(layer.weights) > 0:\n",
        "            # Only consider weight matrices, not biases\n",
        "            weight = layer.get_weights()[0]\n",
        "            all_weights.append(weight)\n",
        "            all_shapes.append(weight.shape)\n",
        "            all_layer_indices.append(i)\n",
        "\n",
        "    # Count total parameters\n",
        "    total_params = sum(w.size for w in all_weights)\n",
        "    num_to_prune = int(total_params * prune_percentage / 100)\n",
        "\n",
        "    print(f\"Total trainable parameters: {total_params}\")\n",
        "    print(f\"Parameters to prune: {num_to_prune} ({prune_percentage}%)\")\n",
        "\n",
        "    # Create masks for each layer (initially all ones)\n",
        "    masks = [np.ones_like(w) for w in all_weights]\n",
        "\n",
        "    # If pruning percentage is not 0, create masks and apply them\n",
        "    if prune_percentage > 0:\n",
        "        # Randomly select indices to prune across all parameters\n",
        "        flat_indices = np.random.choice(total_params, size=num_to_prune, replace=False)\n",
        "\n",
        "        # Map flat indices back to layer, row, col indices\n",
        "        params_so_far = 0\n",
        "        for i, weight in enumerate(all_weights):\n",
        "            size = weight.size\n",
        "            # Get indices that fall within this layer\n",
        "            indices_in_layer = flat_indices[(flat_indices >= params_so_far) &\n",
        "                                             (flat_indices < params_so_far + size)] - params_so_far\n",
        "\n",
        "            # Flatten the mask, set the selected indices to zero, then reshape back\n",
        "            flat_mask = masks[i].flatten()\n",
        "            flat_mask[indices_in_layer] = 0\n",
        "            masks[i] = flat_mask.reshape(all_shapes[i])\n",
        "\n",
        "            params_so_far += size\n",
        "\n",
        "        # Apply masks to each layer's weights\n",
        "        for i, layer_idx in enumerate(all_layer_indices):\n",
        "            layer = model.layers[layer_idx]\n",
        "            weights = layer.get_weights()\n",
        "            weights[0] = weights[0] * masks[i]  # Apply mask to weights\n",
        "            layer.set_weights(weights)\n",
        "\n",
        "    # Create a custom calo enforce the mask during training\n",
        "    class MaskWeightsCallback(tf.keras.callbacks.Callback):\n",
        "        def __init__(self, masks, layer_indices):\n",
        "            self.masks = masks\n",
        "            self.layer_indices = layer_indices\n",
        "\n",
        "        def on_batch_end(self, batch, logs=None): ## here i need to be careful\n",
        "            # Apply masks after each batch update\n",
        "            for i, layer_idx in enumerate(self.layer_indices):\n",
        "                layer = self.model.layers[layer_idx]\n",
        "                weights = layer.get_weights()\n",
        "                weights[0] = weights[0] * self.masks[i]\n",
        "                layer.set_weights(weights)\n",
        "\n",
        "    # Attach the mask callback to the model for later use\n",
        "    model.mask_callback = MaskWeightsCallback(masks, all_layer_indices)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Google Sheets Logger class\n",
        "class GoogleSheetsLogger(Callback):\n",
        "    def __init__(self, test_data, batch_size, spreadsheet=None, sheet_name=None, baseline_ce=None):\n",
        "        \"\"\"\n",
        "        Initialize the Google Sheets Logger\n",
        "\n",
        "        Parameters:\n",
        "        test_data -- tuple of (x_test, y_test)\n",
        "        batch_size -- batch size for evaluation\n",
        "        spreadsheet -- existing Google spreadsheet object to use\n",
        "        sheet_name -- name of the worksheet to create/use within the spreadsheet\n",
        "        baseline_ce -- baseline cross-entropy from unpruned model (for IPA calculation)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.test_x, self.test_y = test_data\n",
        "        self.batch_size = batch_size\n",
        "        self.epoch = 0\n",
        "        self.batch_num = 0\n",
        "        self.total_batches = 0\n",
        "        self.baseline_ce = baseline_ce\n",
        "        self.ln10 = math.log(10)  # Approx 2.302\n",
        "        self.ipa_start_batch = None\n",
        "        self.spreadsheet = spreadsheet\n",
        "\n",
        "        # Use pruning percentage as sheet name if not provided\n",
        "        if sheet_name is None:\n",
        "            sheet_name = f\"Training_Log_{int(time.time())}\"\n",
        "        self.sheet_name = sheet_name\n",
        "\n",
        "        # Column headers for the sheet\n",
        "        self.headers = [\n",
        "            'Epoch', 'Batch Number',\n",
        "            'Train Accur', 'Train CE', 'Test Accur', 'Test CE',\n",
        "            '|CE(b)-CEo|', 'IPA'\n",
        "        ]\n",
        "\n",
        "        # Authenticate and setup Google Sheets\n",
        "        self._setup_google_sheets()\n",
        "\n",
        "    def _setup_google_sheets(self):\n",
        "        \"\"\"Setup authentication and create the Google Sheet\"\"\"\n",
        "        # Authenticate to Google if not already done\n",
        "        if self.spreadsheet is None:\n",
        "            auth.authenticate_user()\n",
        "\n",
        "            # Get credentials (using google-auth instead of oauth2client)\n",
        "            creds, _ = default()\n",
        "\n",
        "            # Create a gspread client\n",
        "            self.gc = gspread.authorize(creds)\n",
        "\n",
        "            try:\n",
        "                # Try to open existing spreadsheet\n",
        "                self.sheet = self.gc.open(\"DenseNet121_Pruning_Experiments\")\n",
        "                print(f\"Using existing spreadsheet: DenseNet121_Pruning_Experiments\")\n",
        "            except gspread.exceptions.SpreadsheetNotFound:\n",
        "                # Create new spreadsheet if it doesn't exist\n",
        "                self.sheet = self.gc.create(\"DenseNet121_Pruning_Experiments\")\n",
        "                print(f\"Created new spreadsheet: DenseNet121_Pruning_Experiments\")\n",
        "\n",
        "                # Share the spreadsheet with the specified email\n",
        "                self.sheet.share(EMAIL, perm_type='user', role='writer')\n",
        "                print(f\"Shared spreadsheet with: {EMAIL}\")\n",
        "\n",
        "            # Store for future use\n",
        "            self.spreadsheet = self.sheet\n",
        "        else:\n",
        "            # Use the provided spreadsheet\n",
        "            self.sheet = self.spreadsheet\n",
        "\n",
        "        # Get or create the worksheet for this pruning percentage\n",
        "        try:\n",
        "            self.worksheet = self.sheet.worksheet(self.sheet_name)\n",
        "            # Clear existing content\n",
        "            self.worksheet.clear()\n",
        "        except gspread.exceptions.WorksheetNotFound:\n",
        "            self.worksheet = self.sheet.add_worksheet(title=self.sheet_name, rows=1000, cols=len(self.headers))\n",
        "\n",
        "        # Add headers to the worksheet\n",
        "        self.worksheet.update('A1', [self.headers])\n",
        "\n",
        "        # Print the URL to access the spreadsheet\n",
        "        print(f\"Spreadsheet URL: {self.sheet.url}\")\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        # If baseline CE is not provided, calculate it\n",
        "        if self.baseline_ce is None:\n",
        "            print(\"Calculating baseline CE...\")\n",
        "            _, _, self.baseline_ce = self.model.evaluate(\n",
        "                self.test_x, self.test_y,\n",
        "                batch_size=self.batch_size,\n",
        "                verbose=1\n",
        "            )\n",
        "            print(f\"Baseline CE (CEo): {self.baseline_ce}\")\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.epoch = epoch + 1\n",
        "        self.batch_num = 0 # remove this to remove the reset of the batch\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        self.batch_num += 1\n",
        "        self.total_batches += 1\n",
        "\n",
        "        # Calculate metrics on test data (silently)\n",
        "        _, test_acc, test_ce = self.model.evaluate(\n",
        "            self.test_x, self.test_y,\n",
        "            batch_size=self.batch_size,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Calculate CE difference\n",
        "        ce_diff = abs(test_ce - self.baseline_ce)\n",
        "\n",
        "        # Determine if we should start IPA calculation\n",
        "        if self.ipa_start_batch is None and test_ce <= self.ln10:\n",
        "            self.ipa_start_batch = self.total_batches\n",
        "            print(f\"Started IPA calculation at batch {self.ipa_start_batch} (CE = {test_ce:.4f})\")\n",
        "\n",
        "        # Calculate IPA only if we've reached the threshold\n",
        "        if self.ipa_start_batch is not None:\n",
        "            # Use batches since we hit the ln10 threshold\n",
        "            adjusted_batches = self.total_batches - self.ipa_start_batch + 1\n",
        "            tl = BATCH_SIZE_TRAIN * adjusted_batches\n",
        "            ipa = ce_diff / tl if tl > 0 else 0\n",
        "            ipa_value = ipa  # Store the actual value\n",
        "        else:\n",
        "            # Before we hit threshold, IPA is marked as N/A\n",
        "            ipa_value = \"N/A\"\n",
        "            ipa = None\n",
        "\n",
        "        # Prepare row to append\n",
        "        row = [\n",
        "            self.epoch,\n",
        "            self.batch_num,\n",
        "            logs.get('accuracy', 0),\n",
        "            logs.get('loss', 0),  # Train CE\n",
        "            test_acc,\n",
        "            test_ce,\n",
        "            ce_diff,\n",
        "            ipa_value\n",
        "        ]\n",
        "\n",
        "        # Append row to Google Sheet (without printing each batch)\n",
        "        try:\n",
        "            self.worksheet.append_row(row)\n",
        "        except Exception as e:\n",
        "            print(f\"Error writing to Google Sheet: {e}\")\n",
        "\n",
        "    def get_final_metrics(self):\n",
        "        \"\"\"Return the final IPA and test accuracy\"\"\"\n",
        "        # If we've identified an IPA starting batch\n",
        "        if self.ipa_start_batch is not None:\n",
        "            # Try to get the latest values from the Google Sheet\n",
        "            try:\n",
        "                values = self.worksheet.get_all_values()\n",
        "                if len(values) > 1:  # If we have data (header + at least one row)\n",
        "                    last_row = values[-1]\n",
        "                    # Check if the IPA is a number\n",
        "                    ipa_value = last_row[7]  # Index 7 is IPA\n",
        "                    if ipa_value != \"N/A\":\n",
        "                        return {\n",
        "                            'Final Test Accuracy': float(last_row[4]),  # Index 4 is Test Accur\n",
        "                            'IPA': float(ipa_value) * 1000  # Scale by 1000\n",
        "                        }\n",
        "            except Exception as e:\n",
        "                print(f\"Error getting final metrics from Google Sheet: {e}\")\n",
        "\n",
        "        # Fall back to calculating final metrics\n",
        "        _, test_acc, test_ce = self.model.evaluate(\n",
        "            self.test_x, self.test_y,\n",
        "            batch_size=self.batch_size,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        ce_diff = abs(test_ce - self.baseline_ce)\n",
        "\n",
        "        if self.ipa_start_batch is not None:\n",
        "            adjusted_batches = self.total_batches - self.ipa_start_batch + 1\n",
        "            tl = BATCH_SIZE_TRAIN * adjusted_batches\n",
        "            ipa = ce_diff / tl if tl > 0 else 0\n",
        "            return {\n",
        "                'Final Test Accuracy': test_acc,\n",
        "                'IPA': ipa * 1000  # Scale by 1000 as requested\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'Final Test Accuracy': test_acc,\n",
        "                'IPA': 0  # If IPA calculation never started\n",
        "            }\n",
        "\n",
        "# Function to run experiment for a dataset and pruning method\n",
        "def run_experiment(dataset_name, pruning_method):\n",
        "    results = []\n",
        "\n",
        "    # Load dataset\n",
        "    (x_train, y_train), (x_test, y_test), num_classes = load_dataset(dataset_name)\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = f\"results_{dataset_name}_{pruning_method}\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # First, get baseline CE from unpruned model\n",
        "    print(\"\\nCreating baseline model to calculate baseline CE...\")\n",
        "    baseline_model = create_densenet121(input_shape=x_train.shape[1:], num_classes=num_classes)\n",
        "\n",
        "    # Compile baseline model\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
        "    baseline_model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', 'categorical_crossentropy']\n",
        "    )\n",
        "\n",
        "    # Calculate baseline CE\n",
        "    _, _, baseline_ce = baseline_model.evaluate(\n",
        "        x_test, y_test,\n",
        "        batch_size=BATCH_SIZE_TEST,\n",
        "        verbose=1\n",
        "    )\n",
        "    print(f\"Baseline CE (CEo): {baseline_ce}\")\n",
        "\n",
        "    # Clear memory\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # Create a summary worksheet for overall results\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "\n",
        "    try:\n",
        "        # Try to open existing spreadsheet\n",
        "        sheet = gc.open(\"DenseNet121_Pruning_Experiments\")\n",
        "    except gspread.exceptions.SpreadsheetNotFound:\n",
        "        # Create new spreadsheet if it doesn't exist\n",
        "        sheet = gc.create(\"DenseNet121_Pruning_Experiments\")\n",
        "        sheet.share(EMAIL, perm_type='user', role='writer')\n",
        "\n",
        "    # Create or update summary worksheet\n",
        "    summary_name = f\"{dataset_name}_{pruning_method}_summary\"\n",
        "    try:\n",
        "        summary_sheet = sheet.worksheet(summary_name)\n",
        "        summary_sheet.clear()\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        summary_sheet = sheet.add_worksheet(title=summary_name, rows=100, cols=3)\n",
        "\n",
        "    # Add headers to summary sheet\n",
        "    summary_sheet.update('A1', [['Prune Percentage', 'Final Test Accuracy', 'IPA × 1000']])\n",
        "\n",
        "    # Save spreadsheet object for reuse\n",
        "    spreadsheet = sheet\n",
        "\n",
        "    for prune_percentage in PRUNE_PERCENTAGES:\n",
        "        print(f\"\\nRunning experiment: {dataset_name}, {pruning_method}, P% = {prune_percentage}\")\n",
        "\n",
        "        # Create model\n",
        "        model = create_densenet121(input_shape=x_train.shape[1:], num_classes=num_classes)\n",
        "\n",
        "        # Count total parameters before pruning\n",
        "        total_params = model.count_params()\n",
        "        print(f\"Total parameters before pruning: {total_params:,}\")\n",
        "\n",
        "        # Apply parameter mask pruning\n",
        "        model = parameter_mask_pruning(model, prune_percentage)\n",
        "        print(f\"Applied parameter mask pruning: P% = {prune_percentage}%\")\n",
        "\n",
        "        # Store the mask callback for use during training\n",
        "        mask_callback = model.mask_callback\n",
        "\n",
        "        # Compile model\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy', 'categorical_crossentropy']\n",
        "        )\n",
        "\n",
        "        # Setup Google Sheets logger\n",
        "        sheet_name = f\"P{prune_percentage}\"\n",
        "        google_sheet_logger = GoogleSheetsLogger(\n",
        "            test_data=(x_test, y_test),\n",
        "            batch_size=BATCH_SIZE_TEST,\n",
        "            spreadsheet=spreadsheet,\n",
        "            sheet_name=sheet_name,\n",
        "            baseline_ce=baseline_ce\n",
        "        )\n",
        "\n",
        "        # Setup callbacks\n",
        "        callbacks = [google_sheet_logger, mask_callback]\n",
        "\n",
        "        # Train for 1 epoch\n",
        "        print(f\"Training with batch size: {BATCH_SIZE_TRAIN}\")\n",
        "        print(f\"Validation batch size: {BATCH_SIZE_TEST}\")\n",
        "        print(f\"Logging to Google Sheet: {sheet_name}\")\n",
        "\n",
        "        # Configure TensorFlow to be less verbose during training\n",
        "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "        history = model.fit(\n",
        "            x_train, y_train,\n",
        "            batch_size=BATCH_SIZE_TRAIN,\n",
        "            validation_batch_size=BATCH_SIZE_TEST,\n",
        "            epochs=5,\n",
        "            validation_data=(x_test, y_test),\n",
        "            callbacks=callbacks,\n",
        "            verbose=2  # Less verbose output\n",
        "        )\n",
        "\n",
        "        # Get final metrics (includes IPA * 1000)\n",
        "        final_metrics = google_sheet_logger.get_final_metrics()\n",
        "\n",
        "        # Add results to summary sheet\n",
        "        summary_sheet.append_row([\n",
        "            prune_percentage,\n",
        "            final_metrics['Final Test Accuracy'],\n",
        "            final_metrics['IPA']\n",
        "        ])\n",
        "\n",
        "        print(f\"Pruning {prune_percentage}% - Test Accuracy: {final_metrics['Final Test Accuracy']:.4f}, IPA × 1000: {final_metrics['IPA']:.6f}\")\n",
        "\n",
        "        results.append({\n",
        "            'Prune Percentage': prune_percentage,\n",
        "            'Final Test Accuracy': final_metrics['Final Test Accuracy'],\n",
        "            'IPA': final_metrics['IPA']\n",
        "        })\n",
        "\n",
        "        # Clear memory between runs\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "    # Create the DataFrame from the results list\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Save results to CSV\n",
        "    results_df.to_csv(os.path.join(output_dir, \"experiment_results.csv\"), index=False)\n",
        "\n",
        "    # Plot IPA vs Pruning Percentage\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(results_df['Prune Percentage'], results_df['IPA'], 'o-')\n",
        "    plt.xlabel('Pruning Percentage (P%)')\n",
        "    plt.ylabel('IPA × 1000')\n",
        "    plt.title(f'IPA vs P% for {dataset_name} using {pruning_method} pruning')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(output_dir, \"ipa_vs_prune.png\"))\n",
        "\n",
        "    # Also plot accuracy vs Pruning Percentage\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(results_df['Prune Percentage'], results_df['Final Test Accuracy'], 'o-')\n",
        "    plt.xlabel('Pruning Percentage (P%)')\n",
        "    plt.ylabel('Final Test Accuracy')\n",
        "    plt.title(f'Accuracy vs P% for {dataset_name} using {pruning_method} pruning')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(output_dir, \"accuracy_vs_prune.png\"))\n",
        "\n",
        "    # Close all plots\n",
        "    plt.close('all')\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Focus only on MNIST dataset\n",
        "    dataset = 'mnist'\n",
        "\n",
        "    # Use parameter_mask pruning\n",
        "    pruning_method = 'parameter_mask'\n",
        "\n",
        "    print(f\"Running DenseNet-121 pruning experiments on {dataset} dataset\")\n",
        "    print(f\"Training batch size: {BATCH_SIZE_TRAIN}, Testing batch size: {BATCH_SIZE_TEST}\")\n",
        "    print(f\"Pruning percentages: {PRUNE_PERCENTAGES}\")\n",
        "    print(f\"Results will be shared with: {EMAIL}\")\n",
        "\n",
        "    # Run parameter mask pruning experiment\n",
        "    print(\"\\nStarting Parameter Mask Pruning experiment...\")\n",
        "    results = run_experiment(dataset, pruning_method)\n",
        "\n",
        "    print(\"\\nExperiment complete!\")\n",
        "    print(f\"Results saved to results_{dataset}_{pruning_method}/experiment_results.csv\")\n",
        "    print(f\"Plots saved to results_{dataset}_{pruning_method}/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqdfZlG6wb14"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}